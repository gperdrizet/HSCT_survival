{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSCT survival: XGBoost ensemble\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import configuration as config\n",
    "import functions.helper as helper_funcs\n",
    "\n",
    "notebook_num='03.3'\n",
    "gpu=0\n",
    "\n",
    "# Data files\n",
    "datasets_file=f'{config.PROCESSED_DATA}/02.1-dataset_definitions.pkl'\n",
    "coxph_survival_file=f'{config.PROCESSED_DATA}/02.1-coxPH_survival.pkl'\n",
    "weibullaft_survival_file=f'{config.PROCESSED_DATA}/02.2-weibullAFT_survival.pkl'\n",
    "learned_efs_file=f'{config.PROCESSED_DATA}/02.4-learned_efs.pkl'\n",
    "\n",
    "# Model files\n",
    "tuned_model_file=f'{config.MODELS_PATH}/{notebook_num}-XGBoost_engineered_features_tuned.pkl'\n",
    "\n",
    "# Experiment results\n",
    "hyperparameter_tuning_results=f'{config.DATA_PATH}/results/data/{notebook_num}-hyperparameter_tuning_results.csv'\n",
    "training_scores_file=f'{config.DATA_PATH}/results/data/{notebook_num}-training_scores.csv'\n",
    "testing_scores_file=f'{config.DATA_PATH}/results/data/{notebook_num}-testing_scores.csv'\n",
    "\n",
    "retune_model=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset metadata\n",
    "with open(datasets_file, 'rb') as input_file:\n",
    "    datasets=pickle.load(input_file)\n",
    "\n",
    "# Load one of the datasets\n",
    "with open(datasets['Nominal one-hot/ordinal encoded, NANs imputed'], 'rb') as input_file:\n",
    "    data_dict=pickle.load(input_file)\n",
    "\n",
    "print('Data dictionary contains:\\n')\n",
    "for key, value in data_dict.items():\n",
    "    print(f' {key}: {type(value)}')\n",
    "\n",
    "# Load Cox Proportional Hazard model features\n",
    "with open(coxph_survival_file, 'rb') as input_file:\n",
    "    coxph_features=pickle.load(input_file)\n",
    "\n",
    "print('\\nCox PH features:\\n')\n",
    "for key, value in coxph_features.items():\n",
    "    print(f' {key}: {type(value)}')\n",
    "\n",
    "# Load Weibull Accelerated Failure Time model features\n",
    "with open(weibullaft_survival_file, 'rb') as input_file:\n",
    "    weibullaft_features=pickle.load(input_file)\n",
    "\n",
    "print('\\nWeibull AFT features:\\n')\n",
    "for key, value in weibullaft_features.items():\n",
    "    print(f' {key}: {type(value)}')\n",
    "\n",
    "# Load learned efs features\n",
    "with open(learned_efs_file, 'rb') as input_file:\n",
    "    learned_efs_features=pickle.load(input_file)\n",
    "\n",
    "print('\\nLearned EFS features:\\n')\n",
    "for key, value in learned_efs_features.items():\n",
    "    print(f' {key}: {type(value)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preparation\n",
    "\n",
    "### 2.1. Add survival model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_df=data_dict['Training features']\n",
    "training_features_df['CoxPH survival']=coxph_features['Training survival']\n",
    "training_features_df['CoxPH partial hazard']=coxph_features['Training partial hazard']\n",
    "training_features_df['WeibullAFT survival']=weibullaft_features['Training survival']\n",
    "training_features_df['WeibullAFT expectation']=weibullaft_features['Training expectation']\n",
    "\n",
    "testing_features_df=data_dict['Testing features']\n",
    "testing_features_df['CoxPH survival']=coxph_features['Testing survival']\n",
    "testing_features_df['CoxPH partial hazard']=coxph_features['Testing partial hazard']\n",
    "testing_features_df['WeibullAFT survival']=weibullaft_features['Testing survival']\n",
    "testing_features_df['WeibullAFT expectation']=weibullaft_features['Testing expectation']\n",
    "\n",
    "training_features_df.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Add learned EFS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_features_df['learned_efs']=learned_efs_features['Training efs probability']\n",
    "# testing_features_df['learned_efs']=learned_efs_features['Testing efs probability']\n",
    "\n",
    "# training_features_df.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Load labels, race group and ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_df=data_dict['Training labels']\n",
    "training_labels_df['efs_time']=np.log(training_labels_df['efs_time'])\n",
    "training_race_groups=data_dict['Training race group']\n",
    "training_ids=data_dict['Training IDs']\n",
    "\n",
    "testing_labels_df=data_dict['Testing labels']\n",
    "testing_labels_df['efs_time']=np.log(testing_labels_df['efs_time'])\n",
    "testing_race_groups=data_dict['Testing race group']\n",
    "testing_ids=data_dict['Testing IDs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoost regression ensemble model\n",
    "\n",
    "### 3.1. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(training_features_df:pd.DataFrame, training_labels_df:pd.DataFrame, label:str, n:int, hyperparameters:dict) -> list:\n",
    "\n",
    "    working_training_features_df=training_features_df.copy()\n",
    "    working_training_labels_df=training_labels_df.copy()\n",
    "\n",
    "    splitter=ShuffleSplit(n_splits=1, test_size=.25)\n",
    "    models=[]\n",
    "\n",
    "    for i in range(n):\n",
    "        print(f'Fold {i}')\n",
    "        \n",
    "        training_df=pd.concat([working_training_features_df, working_training_labels_df], axis=1)\n",
    "        \n",
    "        for training_idx, validation_idx in splitter.split(training_df):\n",
    "\n",
    "            # Get the features for this fold\n",
    "            training_features=training_df.iloc[training_idx].drop(['efs', 'efs_time'], axis=1)\n",
    "            validation_features=training_df.iloc[validation_idx].drop(['efs', 'efs_time'], axis=1)\n",
    "\n",
    "            # Get the labels\n",
    "            training_labels=training_df.iloc[training_idx][label]\n",
    "            validation_labels=training_df.iloc[validation_idx][label]\n",
    "\n",
    "            dtraining=xgb.DMatrix(\n",
    "                training_features,\n",
    "                label=training_labels\n",
    "            )\n",
    "\n",
    "            dvalidation=xgb.DMatrix(\n",
    "                validation_features,\n",
    "                label=validation_labels\n",
    "            )\n",
    "\n",
    "            tree_model=xgb.train(\n",
    "                hyperparameters,\n",
    "                dtraining,\n",
    "                num_boost_round=10000,\n",
    "                evals=[(dvalidation, 'validation')],\n",
    "                early_stopping_rounds=500,\n",
    "                verbose_eval=1\n",
    "            )\n",
    "\n",
    "            models.append(tree_model)\n",
    "\n",
    "            dtraining=xgb.DMatrix(\n",
    "                working_training_features_df\n",
    "            )\n",
    "\n",
    "            predictions=tree_model.predict(dtraining)\n",
    "            working_training_features_df[f'predictions_{i}']=predictions\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hyperparameters={\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 6,\n",
    "    'gamma': 0.01,\n",
    "    'subsample': 0.5\n",
    "}\n",
    "\n",
    "if gpu != None:\n",
    "    hyperparameters['gpu_id']=gpu\n",
    "    hyperparameters['tree_method']='gpu_hist'\n",
    "\n",
    "models=train_ensemble(training_features_df, training_labels_df, 'efs_time', 10, hyperparameters)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_features=testing_features_df.copy()\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "\n",
    "    dtesting=xgb.DMatrix(testing_features)\n",
    "    predictions=model.predict(dtesting)\n",
    "    testing_features[f'predictions_{i}']=predictions\n",
    "\n",
    "# Calculate fit residuals\n",
    "testing_residuals=predictions - testing_labels_df['efs_time']\n",
    "\n",
    "fig, axs=plt.subplots(1,2, figsize=(9,4))\n",
    "axs=axs.flatten()\n",
    "\n",
    "axs[0].set_title('Testing predictions')\n",
    "axs[0].scatter(testing_labels_df['efs_time'], predictions, color='black', s=0.2)\n",
    "axs[0].set_xlabel('True EFS time')\n",
    "axs[0].set_ylabel('Predicted EFS time')\n",
    "\n",
    "axs[1].set_title('Testing residuals')\n",
    "axs[1].scatter(predictions, testing_residuals, color='black', s=0.2)\n",
    "axs[1].set_xlabel('Predicted EFS time')\n",
    "axs[1].set_ylabel('EFS time residual')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### 4.3. Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_results=helper_funcs.score_predictions(\n",
    "    'Testing predictions',\n",
    "    predictions,\n",
    "    testing_labels_df['efs_time'].values,\n",
    "    testing_labels_df['efs'].values,\n",
    "    testing_race_groups,\n",
    "    testing_ids\n",
    ")\n",
    "\n",
    "score_results=helper_funcs.score_predictions(\n",
    "    'Labels',\n",
    "    testing_labels_df['efs_time'].values,\n",
    "    testing_labels_df['efs_time'].values,\n",
    "    testing_labels_df['efs'].values,\n",
    "    testing_race_groups,\n",
    "    testing_ids,\n",
    "    results=score_results\n",
    ")\n",
    "\n",
    "score_results_df=pd.DataFrame(score_results)\n",
    "score_results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost classification ensemble\n",
    "\n",
    "### 4.1. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Calculated class weighting\n",
    "class_weight=(len(training_labels_df) - sum(training_labels_df['efs'])) / sum(training_labels_df['efs'])\n",
    "\n",
    "hyperparameters={\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'scale_pos_weight': class_weight,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 6,\n",
    "    'gamma': 0.01,\n",
    "    'subsample': 0.5\n",
    "}\n",
    "\n",
    "if gpu != None:\n",
    "    hyperparameters['gpu_id']=gpu\n",
    "    hyperparameters['tree_method']='gpu_hist'\n",
    "\n",
    "models=train_ensemble(training_features_df, training_labels_df, 'efs', 10, hyperparameters)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_features=testing_features_df.copy()\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "\n",
    "    dtesting=xgb.DMatrix(testing_features)\n",
    "    predictions=model.predict(dtesting)\n",
    "    testing_features[f'predictions_{i}']=predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(1,2, figsize=(9,3.5))\n",
    "axs=axs.flatten()\n",
    "\n",
    "# Make calls with threshold\n",
    "calls=np.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm=confusion_matrix(testing_labels_df['efs'], calls, normalize='true')\n",
    "cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "_=cm_disp.plot(ax=axs[0])\n",
    "\n",
    "axs[0].set_title('Classification performance')\n",
    "axs[0].set_xlabel('Predicted EFS')\n",
    "axs[0].set_ylabel('True EFS')\n",
    "\n",
    "axs[1].set_title('Classification probability')\n",
    "axs[1].hist(predictions, bins=30, color='black')\n",
    "axs[1].set_xlabel('Probability')\n",
    "axs[1].set_ylabel('Count')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSCT survival: data cleaning and encoding\n",
    "\n",
    "## 1. Notebook set-up\n",
    "\n",
    "### 1.1. Imports & options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyPI imports\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config\n",
    "import functions.encoding as encode_funcs\n",
    "import functions.helper as helper_funcs\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "rebuild_datasets=False\n",
    "\n",
    "# Some cleaning/encoding options\n",
    "knn_neighbors=5\n",
    "one_hot_drop, collinearity='first', 'no-multicollinearity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base data input file: ID column set as index, missing string placeholders converted to nan\n",
    "# ordinal categorical features translated to numerical categorical where possible\n",
    "translated_features_file=f'{config.PROCESSED_DATA}/01.1-features_translated.pkl'\n",
    "\n",
    "# Feature data type definition file\n",
    "feature_types_dict_file=f'{config.PROCESSED_DATA}/01.1-feature_type_dict.pkl'\n",
    "\n",
    "# Nominal and ordinal features ordinal encoded.\n",
    "# NAN values encoded as 'missing' for categorical features\n",
    "# and KNN imputed for numerical features.\n",
    "ordinal_all_nan_encoded_data_df_file=f'{config.PROCESSED_DATA}/02.1-{collinearity}_ordinal_all_nan_encoded_data_df.pkl'\n",
    "\n",
    "# Nominal and ordinal features ordinal encoded encoded.\n",
    "# NANs filled in by KNN imputation for all features.\n",
    "ordinal_all_nan_imputed_data_df_file=f'{config.PROCESSED_DATA}/02.1-{collinearity}_ordinal_all_nan_imputed_data_df.pkl'\n",
    "\n",
    "# Nominal features one hot encoded, ordinal features ordinal encoded. \n",
    "# Data with with NAN values encoded as missing for categorical features \n",
    "# and KNN imputed for numerical features.\n",
    "one_hot_ordinal_nan_encoded_data_df_file=f'{config.PROCESSED_DATA}/02.1-{collinearity}_one_hot_ordinal_nan_encoded_data_df.pkl'\n",
    "\n",
    "# Nominal features one hot encoded, ordinal features ordinal encoded.\n",
    "# NANs filled in by KNN imputation for all features.\n",
    "one_hot_ordinal_nan_imputed_data_df_file=f'{config.PROCESSED_DATA}/02.1-{collinearity}_one_hot_ordinal_nan_imputed_data_df.pkl'\n",
    "\n",
    "# All ordinal and nominal features target encoded on EFS\n",
    "binary_target_encoded_data_file=f'{config.PROCESSED_DATA}/02.1-binary_target_encoded_data_df.pkl'\n",
    "\n",
    "# All ordinal and nominal features target encoded on EFS time\n",
    "continuous_target_encoded_data_file=f'{config.PROCESSED_DATA}/02.1-continuous_target_encoded_data_df.pkl'\n",
    "\n",
    "# Save dataset definitions\n",
    "datasets={\n",
    "    'Nominal one-hot/ordinal encoded, NANs encoded':one_hot_ordinal_nan_encoded_data_df_file,\n",
    "    'Nominal one-hot/ordinal encoded, NANs imputed':one_hot_ordinal_nan_imputed_data_df_file,\n",
    "    'All ordinal encoded, NAN encoded':ordinal_all_nan_encoded_data_df_file,\n",
    "    'All ordinal encoded, NAN imputed':ordinal_all_nan_imputed_data_df_file,\n",
    "    'Binary target encoded':binary_target_encoded_data_file,\n",
    "    'Continuous target encoded':continuous_target_encoded_data_file\n",
    "}\n",
    "\n",
    "# Dataset definition file\n",
    "datasets_file=f'{config.PROCESSED_DATA}/02.1-dataset_definitions.pkl'\n",
    "\n",
    "# Save the dataset metadata\n",
    "with open(datasets_file, 'wb') as output_file:\n",
    "    pickle.dump(datasets, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input data\n",
    "\n",
    "### 2.1. Feature type definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature data type definitions\n",
    "with open(feature_types_dict_file, 'rb') as input_file:\n",
    "    feature_types_dict=pickle.load(input_file)\n",
    "\n",
    "print('Feature types:\\n')\n",
    "\n",
    "for feature_type, features in feature_types_dict.items():\n",
    "    print(f'{feature_type}\\n{features}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rebuild_datasets == True:\n",
    "    \n",
    "    # Load the dataset\n",
    "    data_df=pd.read_parquet(translated_features_file)\n",
    "\n",
    "    # Train test split\n",
    "    master_training_df, master_testing_df=train_test_split(data_df, test_size=0.3, random_state=315)\n",
    "\n",
    "    # Save the un-encoded race group so we can use it in C-index scoring later\n",
    "    training_race_group=master_training_df['race_group']\n",
    "    testing_race_group=master_testing_df['race_group']\n",
    "\n",
    "    # Save and remove the IDs\n",
    "    training_ids=master_training_df.index\n",
    "    master_training_df.reset_index(drop=True, inplace=True)\n",
    "    testing_ids=master_testing_df.index\n",
    "    master_testing_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Remove the labels\n",
    "    master_training_labels_df=master_training_df[['efs', 'efs_time']].copy()\n",
    "    master_training_features_df=master_training_df.drop(['efs', 'efs_time'], axis=1)\n",
    "    master_testing_labels_df=master_testing_df[['efs', 'efs_time']].copy()\n",
    "    master_testing_features_df=master_testing_df.drop(['efs', 'efs_time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoding and NAN handling schemes\n",
    "\n",
    "### 3.1. One-hot encode nominal features with missing value string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rebuild_datasets == True:\n",
    "    \n",
    "    # Make a copy of the master input testing and training features\n",
    "    training_df=master_training_features_df.copy()\n",
    "    testing_df=master_testing_features_df.copy()\n",
    "\n",
    "    # Encode the nominal features\n",
    "    nominal_training_df, nominal_testing_df=encode_funcs.one_hot_nan_encoded(\n",
    "        training_df=training_df,\n",
    "        testing_df=testing_df,\n",
    "        features=feature_types_dict['Nominal'],\n",
    "        models_path=config.MODELS_PATH\n",
    "    )\n",
    "\n",
    "    # Encode the ordinal features\n",
    "    ordinal_training_df, ordinal_testing_df=encode_funcs.ordinal_encode_nan_imputed(\n",
    "        training_df=training_df,\n",
    "        testing_df=testing_df,\n",
    "        features=feature_types_dict['Ordinal'],\n",
    "        models_path=config.MODELS_PATH\n",
    "    )\n",
    "\n",
    "    # Clean NANs in the interval features\n",
    "    interval_training_df, interval_testing_df=encode_funcs.impute_numerical_features(\n",
    "        training_df=training_df,\n",
    "        testing_df=testing_df,\n",
    "        features=feature_types_dict['Interval'],\n",
    "        models_path=config.MODELS_PATH\n",
    "    )\n",
    "\n",
    "    # Join the data back together\n",
    "    training_df=pd.concat([nominal_training_df, ordinal_training_df, interval_training_df], axis=1)\n",
    "    testing_df=pd.concat([nominal_testing_df, ordinal_testing_df, interval_testing_df], axis=1)\n",
    "    print(f'Re-combined data: {training_df.shape}')\n",
    "\n",
    "    # Assemble dataset dictionary\n",
    "    dataset={\n",
    "        'Training features': training_df,\n",
    "        'Training labels': master_training_labels_df,\n",
    "        'Training IDs': training_ids,\n",
    "        'Training race group': training_race_group,\n",
    "        'Testing features': testing_df,\n",
    "        'Testing labels': master_testing_labels_df,\n",
    "        'Testing IDs': testing_ids,\n",
    "        'Testing race group': testing_race_group,\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    with open(one_hot_ordinal_nan_encoded_data_df_file, 'wb') as output_file:\n",
    "        pickle.dump(dataset, output_file)\n",
    "\n",
    "    # Inspect\n",
    "    training_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. One-hot encode nominal features with missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rebuild_datasets == True:\n",
    "\n",
    "    # Make a copy of the master input testing and training features\n",
    "    training_df=master_training_features_df.copy()\n",
    "    testing_df=master_testing_features_df.copy()\n",
    "\n",
    "    # Encode the nominal features\n",
    "    nominal_training_df, nominal_testing_df=encode_funcs.one_hot_encode_nan_imputed(\n",
    "        training_df=training_df,\n",
    "        testing_df=testing_df,\n",
    "        features=feature_types_dict['Nominal'],\n",
    "        models_path=config.MODELS_PATH\n",
    "    )\n",
    "\n",
    "    # Encode the ordinal features\n",
    "    ordinal_training_df, ordinal_testing_df=encode_funcs.ordinal_encode_nan_imputed(\n",
    "        training_df=training_df,\n",
    "        testing_df=testing_df,\n",
    "        features=feature_types_dict['Ordinal'],\n",
    "        models_path=config.MODELS_PATH\n",
    "    )\n",
    "\n",
    "    # Clean NANs in the interval features\n",
    "    interval_training_df, interval_testing_df=encode_funcs.impute_numerical_features(\n",
    "        training_df=training_df,\n",
    "        testing_df=testing_df,\n",
    "        features=feature_types_dict['Interval'],\n",
    "        models_path=config.MODELS_PATH\n",
    "    )\n",
    "\n",
    "    # Join the data back together\n",
    "    training_df=pd.concat([nominal_training_df, ordinal_training_df, interval_training_df], axis=1)\n",
    "    testing_df=pd.concat([nominal_testing_df, ordinal_testing_df, interval_testing_df], axis=1)\n",
    "    print(f'Re-combined data: {training_df.shape}')\n",
    "\n",
    "    # Assemble dataset dictionary\n",
    "    dataset={\n",
    "        'Training features': training_df,\n",
    "        'Training labels': master_training_labels_df,\n",
    "        'Training IDs': training_ids,\n",
    "        'Training race group': training_race_group,\n",
    "        'Testing features': testing_df,\n",
    "        'Testing labels': master_testing_labels_df,\n",
    "        'Testing IDs': testing_ids,\n",
    "        'Testing race group': testing_race_group,\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    with open(one_hot_ordinal_nan_imputed_data_df_file, 'wb') as output_file:\n",
    "        pickle.dump(dataset, output_file)\n",
    "\n",
    "    # Inspect\n",
    "    training_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Ordinal encode nominal and ordinal features with 'missing' level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rebuild_datasets == True:\n",
    "    \n",
    "    # Make a copy of the master input testing and training features\n",
    "    training_df=master_training_features_df.copy()\n",
    "    testing_df=master_testing_features_df.copy()\n",
    "\n",
    "    # Encode the nominal & ordinal features\n",
    "    categorical_training_df, categorical_testing_df=encode_funcs.ordinal_encode_nan_encoded(\n",
    "        training_df=training_df,\n",
    "        testing_df=testing_df,\n",
    "        features=feature_types_dict['Nominal'] + feature_types_dict['Ordinal'],\n",
    "        models_path=config.MODELS_PATH\n",
    "    )\n",
    "\n",
    "    # Clean NANs in the interval features\n",
    "    interval_training_df, interval_testing_df=encode_funcs.impute_numerical_features(\n",
    "        training_df=training_df,\n",
    "        testing_df=testing_df,\n",
    "        features=feature_types_dict['Interval'],\n",
    "        models_path=config.MODELS_PATH\n",
    "    )\n",
    "\n",
    "    # Join the data back together\n",
    "    training_df=pd.concat([categorical_training_df, interval_training_df], axis=1)\n",
    "    testing_df=pd.concat([categorical_testing_df, interval_testing_df], axis=1)\n",
    "    print(f'Re-combined data: {training_df.shape}')\n",
    "\n",
    "    # Assemble dataset dictionary\n",
    "    dataset={\n",
    "        'Training features': training_df,\n",
    "        'Training labels': master_training_labels_df,\n",
    "        'Training IDs': training_ids,\n",
    "        'Training race group': training_race_group,\n",
    "        'Testing features': testing_df,\n",
    "        'Testing labels': master_testing_labels_df,\n",
    "        'Testing IDs': testing_ids,\n",
    "        'Testing race group': testing_race_group,\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    with open(ordinal_all_nan_encoded_data_df_file, 'wb') as output_file:\n",
    "        pickle.dump(dataset, output_file)\n",
    "\n",
    "    # Inspect\n",
    "    training_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Ordinal encode nominal and ordinal features with NAN imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rebuild_datasets == True:\n",
    "\n",
    "    # Make a copy of the master input testing and training features\n",
    "    training_df=master_training_features_df.copy()\n",
    "    testing_df=master_testing_features_df.copy()\n",
    "\n",
    "    # Encode the nominal & ordinal features\n",
    "    categorical_training_df, categorical_testing_df=encode_funcs.ordinal_encode_nan_imputed(\n",
    "        training_df=training_df,\n",
    "        testing_df=testing_df,\n",
    "        features=feature_types_dict['Nominal'] + feature_types_dict['Ordinal'],\n",
    "        models_path=config.MODELS_PATH\n",
    "    )\n",
    "\n",
    "    # Clean NANs in the interval features\n",
    "    interval_training_df, interval_testing_df=encode_funcs.impute_numerical_features(\n",
    "        training_df=training_df,\n",
    "        testing_df=testing_df,\n",
    "        features=feature_types_dict['Interval'],\n",
    "        models_path=config.MODELS_PATH\n",
    "    )\n",
    "\n",
    "    # Join the data back together\n",
    "    training_df=pd.concat([categorical_training_df, interval_training_df], axis=1)\n",
    "    testing_df=pd.concat([categorical_testing_df, interval_testing_df], axis=1)\n",
    "    print(f'Re-combined data: {training_df.shape}')\n",
    "\n",
    "    # Assemble dataset dictionary\n",
    "    dataset={\n",
    "        'Training features': training_df,\n",
    "        'Training labels': master_training_labels_df,\n",
    "        'Training IDs': training_ids,\n",
    "        'Training race group': training_race_group,\n",
    "        'Testing features': testing_df,\n",
    "        'Testing labels': master_testing_labels_df,\n",
    "        'Testing IDs': testing_ids,\n",
    "        'Testing race group': testing_race_group,\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    with open(ordinal_all_nan_imputed_data_df_file, 'wb') as output_file:\n",
    "        pickle.dump(dataset, output_file)\n",
    "\n",
    "    # Inspect\n",
    "    training_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Binary target encode everything on efs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rebuild_datasets == True:\n",
    "\n",
    "    # Make a copy of the master input testing and training features\n",
    "    training_df=master_training_features_df.copy()\n",
    "    testing_df=master_testing_features_df.copy()\n",
    "\n",
    "    # Get categorical features\n",
    "    categorical_training_df=training_df[feature_types_dict['Nominal'] + feature_types_dict['Ordinal']]\n",
    "    categorical_testing_df=testing_df[feature_types_dict['Nominal'] + feature_types_dict['Ordinal']]\n",
    "    \n",
    "    # Encode the nominal & ordinal features\n",
    "    encoder=TargetEncoder()\n",
    "    encoder.fit(categorical_training_df, master_training_labels_df['efs'])\n",
    "    encoded_categorical_training_features=encoder.transform(categorical_training_df)\n",
    "    encoded_categorical_testing_features=encoder.transform(categorical_testing_df)\n",
    "\n",
    "    # Save the encoder\n",
    "    with open(f'{config.MODELS_PATH}/01.2-binary_target_encoder.pkl', 'wb') as output_file:\n",
    "        pickle.dump(encoder, output_file)\n",
    "\n",
    "    # Rebuild the dataframes\n",
    "    encoded_categorical_training_features_df=pd.DataFrame(\n",
    "        encoded_categorical_training_features,\n",
    "        columns=feature_types_dict['Nominal'] + feature_types_dict['Ordinal']\n",
    "    )\n",
    "\n",
    "    encoded_categorical_testing_features_df=pd.DataFrame(\n",
    "        encoded_categorical_testing_features,\n",
    "        columns=feature_types_dict['Nominal'] + feature_types_dict['Ordinal']\n",
    "    )\n",
    "\n",
    "    # Clean NANs in the interval features\n",
    "    interval_training_df, interval_testing_df=encode_funcs.impute_numerical_features(\n",
    "        training_df=training_df,\n",
    "        testing_df=testing_df,\n",
    "        features=feature_types_dict['Interval'],\n",
    "        models_path=config.MODELS_PATH\n",
    "    )\n",
    "\n",
    "    # Join the data back together\n",
    "    training_df=pd.concat([encoded_categorical_training_features_df, interval_training_df], axis=1)\n",
    "    testing_df=pd.concat([encoded_categorical_testing_features_df, interval_testing_df], axis=1)\n",
    "    print(f'Re-combined data: {training_df.shape}')\n",
    "\n",
    "    # Assemble dataset dictionary\n",
    "    dataset={\n",
    "        'Training features': training_df,\n",
    "        'Training labels': master_training_labels_df,\n",
    "        'Training IDs': training_ids,\n",
    "        'Training race group': training_race_group,\n",
    "        'Testing features': testing_df,\n",
    "        'Testing labels': master_testing_labels_df,\n",
    "        'Testing IDs': testing_ids,\n",
    "        'Testing race group': testing_race_group,\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    with open(binary_target_encoded_data_file, 'wb') as output_file:\n",
    "        pickle.dump(dataset, output_file)\n",
    "\n",
    "    # Inspect\n",
    "    training_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Continuous target encode everything on efs_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rebuild_datasets == True:\n",
    "\n",
    "    # Make a copy of the master input testing and training features\n",
    "    training_df=master_training_features_df.copy()\n",
    "    testing_df=master_testing_features_df.copy()\n",
    "\n",
    "    # Get categorical features\n",
    "    categorical_training_df=training_df[feature_types_dict['Nominal'] + feature_types_dict['Ordinal']]\n",
    "    categorical_testing_df=testing_df[feature_types_dict['Nominal'] + feature_types_dict['Ordinal']]\n",
    "    \n",
    "    # Encode the nominal & ordinal features\n",
    "    encoder=TargetEncoder()\n",
    "    encoder.fit(categorical_training_df, master_training_labels_df['efs_time'])\n",
    "    encoded_categorical_training_features=encoder.transform(categorical_training_df)\n",
    "    encoded_categorical_testing_features=encoder.transform(categorical_testing_df)\n",
    "\n",
    "    # Save the encoder\n",
    "    with open(f'{config.MODELS_PATH}/01.2-binary_target_encoder.pkl', 'wb') as output_file:\n",
    "        pickle.dump(encoder, output_file)\n",
    "\n",
    "    # Rebuild the dataframes\n",
    "    encoded_categorical_training_features_df=pd.DataFrame(\n",
    "        encoded_categorical_training_features,\n",
    "        columns=feature_types_dict['Nominal'] + feature_types_dict['Ordinal']\n",
    "    )\n",
    "\n",
    "    encoded_categorical_testing_features_df=pd.DataFrame(\n",
    "        encoded_categorical_testing_features,\n",
    "        columns=feature_types_dict['Nominal'] + feature_types_dict['Ordinal']\n",
    "    )\n",
    "\n",
    "    # Clean NANs in the interval features\n",
    "    interval_training_df, interval_testing_df=encode_funcs.impute_numerical_features(\n",
    "        training_df=training_df,\n",
    "        testing_df=testing_df,\n",
    "        features=feature_types_dict['Interval'],\n",
    "        models_path=config.MODELS_PATH\n",
    "    )\n",
    "\n",
    "    # Join the data back together\n",
    "    training_df=pd.concat([encoded_categorical_training_features_df, interval_training_df], axis=1)\n",
    "    testing_df=pd.concat([encoded_categorical_testing_features_df, interval_testing_df], axis=1)\n",
    "    print(f'Re-combined data: {training_df.shape}')\n",
    "\n",
    "    # Assemble dataset dictionary\n",
    "    dataset={\n",
    "        'Training features': training_df,\n",
    "        'Training labels': master_training_labels_df,\n",
    "        'Training IDs': training_ids,\n",
    "        'Training race group': training_race_group,\n",
    "        'Testing features': testing_df,\n",
    "        'Testing labels': master_testing_labels_df,\n",
    "        'Testing IDs': testing_ids,\n",
    "        'Testing race group': testing_race_group,\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    with open(continuous_target_encoded_data_file, 'wb') as output_file:\n",
    "        pickle.dump(dataset, output_file)\n",
    "\n",
    "    # Inspect\n",
    "    training_df.info(verbose=True, show_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

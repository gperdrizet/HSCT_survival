{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSCT survival: clustering\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "\n",
    "import configuration as config\n",
    "import functions.helper as helper_funcs\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Input files\n",
    "\n",
    "# Dataset definition file\n",
    "datasets_definitions_file=f'{config.DATA_PATH}/processed/02.1-dataset_definitions.pkl'\n",
    "\n",
    "# Feature type definitions file\n",
    "feature_types_dict_file=f'{config.DATA_PATH}/processed/01.1-feature_type_dict.pkl'\n",
    "\n",
    "# Output files\n",
    "\n",
    "# EFS+/- split regression results\n",
    "efs_split_regression_results_file=f'{config.DATA_PATH}/results/02.2-efs_split_regression_results.pkl'\n",
    "\n",
    "# Testing of different encoding strategies datasets for clustering\n",
    "datasets_clustering_results_file=f'{config.DATA_PATH}/results/02.2-datasets_clustering_results.pkl'\n",
    "\n",
    "# Testing regression on predicted EFS clusters\n",
    "clustered_regression_results_file=f'{config.DATA_PATH}/results/02.2-clustered_regression_results.pkl'\n",
    "\n",
    "# Run parameters\n",
    "\n",
    "randomsearch_depth=500\n",
    "\n",
    "# Define the hyperparameter search space for gradient boosting trees\n",
    "distributions={\n",
    "    'learning_rate': stats.uniform(loc=0.0, scale=1.0),\n",
    "    'max_iter': list(range(10, 5000)),\n",
    "    'max_leaf_nodes': list(range(2, 1000)),\n",
    "    'max_depth': list(range(2, 1000)),\n",
    "    'min_samples_leaf': list(range(1, 1000)),\n",
    "    'l2_regularization': stats.uniform(loc=0.0, scale=1.0),\n",
    "    'max_features': stats.uniform(loc=0.1, scale=0.9),\n",
    "    'max_bins': list(range(2, 255)),\n",
    "    'interaction_cst': ['pairwise', 'no_interactions']\n",
    "}\n",
    "\n",
    "nn_epochs=500\n",
    "\n",
    "test_split_regression=True\n",
    "test_data_tree_clustering=True\n",
    "clustered_regression_test=True\n",
    "train_nn=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset definitions\n",
    "with open(datasets_definitions_file, 'rb') as input_file:\n",
    "    datasets=pickle.load(input_file)\n",
    "\n",
    "print('Datasets:\\n')\n",
    "for description, filepath in datasets.items():\n",
    "    print(f' {description}')\n",
    "\n",
    "# Load feature definitions\n",
    "with open(feature_types_dict_file, 'rb') as input_file:\n",
    "    feature_types=pickle.load(input_file)\n",
    "\n",
    "print('\\nFeature types:\\n')\n",
    "for feature_type, features in feature_types.items():\n",
    "    print(f' {feature_type}: {features}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Does knowing the EFS label give better regression results?\n",
    "\n",
    "### 2.1. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_split_regression == True:\n",
    "    \n",
    "    efs_test_datasets={}\n",
    "\n",
    "    # Pick and load the 'best' data\n",
    "    data_df=pd.read_parquet(datasets['All ordinal encoded, NAN imputed'])\n",
    "\n",
    "    # Take log of efs time\n",
    "    data_df['efs_time']=np.log(data_df['efs_time'])\n",
    "\n",
    "    # Split the data by EFS label\n",
    "    efs_zero_df=data_df[data_df['efs'] == 0].copy()\n",
    "    efs_one_df=data_df[data_df['efs'] == 1].copy()\n",
    "\n",
    "    # Copy the original data\n",
    "    combined_df=data_df.copy()\n",
    "\n",
    "    efs_test_datasets={\n",
    "        'Combined': combined_df,\n",
    "        'EFS 0': efs_zero_df,\n",
    "        'EFS 1': efs_one_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Cross-validation with test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_split_regression == True:\n",
    "    \n",
    "    cross_val_results={\n",
    "        'Dataset':[],\n",
    "        'Mean RMSE': [],\n",
    "        'Standard deviation RMSE': []\n",
    "    }\n",
    "\n",
    "    # Define cross-validation strategy\n",
    "    cv=ShuffleSplit(n_splits=10, test_size=0.3, random_state=315)\n",
    "\n",
    "    # Loop on the datasets\n",
    "    for dataset, dataset_df in efs_test_datasets.items():\n",
    "\n",
    "        # Drop the race group\n",
    "        training_df=dataset_df.drop('race_group', axis=1, inplace=False)\n",
    "\n",
    "        # Make features and labels\n",
    "        labels_df=training_df[['efs', 'efs_time']].copy()\n",
    "        features_df=training_df.drop(['efs', 'efs_time'], axis=1, inplace=False)\n",
    "\n",
    "        scores=cross_val_score(\n",
    "            HistGradientBoostingRegressor(random_state=315),\n",
    "            features_df,\n",
    "            labels_df['efs_time'],\n",
    "            cv=cv,\n",
    "            scoring='neg_root_mean_squared_error'\n",
    "        )\n",
    "\n",
    "        scores_mean=np.array(abs(scores)).mean()\n",
    "        scores_std=np.array(abs(scores)).std()\n",
    "        cross_val_results['Dataset'].append(dataset)\n",
    "        cross_val_results['Mean RMSE'].append(scores_mean)\n",
    "        cross_val_results['Standard deviation RMSE'].append(scores_std)\n",
    "\n",
    "    cross_val_results_df=pd.DataFrame.from_dict(cross_val_results)\n",
    "    efs_split_regression_results={'Cross-validation scores': cross_val_results_df}\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load last result\n",
    "    with open(efs_split_regression_results_file, 'rb') as input_file:\n",
    "        efs_split_regression_results=pickle.load(input_file)\n",
    "\n",
    "    cross_val_results_df=efs_split_regression_results['Cross-validation scores']\n",
    "\n",
    "cross_val_results_df.head(len(cross_val_results_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Hyperparameter tuning on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_split_regression == True:\n",
    "    \n",
    "    predictions={}\n",
    "    labels={}\n",
    "    best_parameters={}\n",
    "    models={}\n",
    "    race_groups={}\n",
    "\n",
    "    # Define the cross-validation strategy\n",
    "    cross_validation=ShuffleSplit(n_splits=10, test_size=0.3, random_state=315)\n",
    "\n",
    "    # Loop on the datasets\n",
    "    for dataset, dataset_df in efs_test_datasets.items():\n",
    "\n",
    "        # Train test split\n",
    "        training_df, testing_df=train_test_split(dataset_df, test_size=0.3, random_state=315)\n",
    "\n",
    "        # Save the testing race group and drop from training and testing\n",
    "        race_groups[dataset]=testing_df['race_group']\n",
    "        training_df.drop('race_group', axis=1, inplace=True)\n",
    "        testing_df.drop('race_group', axis=1, inplace=True)\n",
    "\n",
    "        # Remove the labels\n",
    "        training_labels_df=training_df[['efs', 'efs_time']].copy()\n",
    "        training_features_df=training_df.drop(['efs', 'efs_time'], axis=1)\n",
    "        testing_labels_df=testing_df[['efs', 'efs_time']].copy()\n",
    "        testing_features_df=testing_df.drop(['efs', 'efs_time'], axis=1)\n",
    "\n",
    "        # Instantiate the model\n",
    "        tree_model=HistGradientBoostingRegressor(random_state=315)\n",
    "\n",
    "        # Set-up the search\n",
    "        search=RandomizedSearchCV(\n",
    "            tree_model,\n",
    "            distributions,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            cv=cross_validation,\n",
    "            n_iter=randomsearch_depth,\n",
    "            random_state=315,\n",
    "            return_train_score=True\n",
    "        )\n",
    "\n",
    "        search_results=search.fit(training_features_df, training_labels_df['efs_time'])\n",
    "        best_parameters[dataset]=search_results.best_params_\n",
    "\n",
    "        # Train classifier with best hyperparameters on complete training set\n",
    "        tree_model=HistGradientBoostingRegressor(**search_results.best_params_, random_state=315)\n",
    "        result=tree_model.fit(training_features_df, training_labels_df['efs_time'])\n",
    "        models[dataset]=tree_model\n",
    "\n",
    "        # Make testing predictions\n",
    "        testing_predictions=tree_model.predict(testing_features_df)\n",
    "        predictions[dataset]=testing_predictions\n",
    "        labels[dataset]=testing_labels_df\n",
    "\n",
    "    efs_split_regression_results['Testing predictions']=predictions\n",
    "    efs_split_regression_results['Testing labels']=labels\n",
    "    efs_split_regression_results['Best parameters']=best_parameters\n",
    "    efs_split_regression_results['Tuned models']=models\n",
    "    efs_split_regression_results['Race groups']=race_groups\n",
    "\n",
    "    with open(efs_split_regression_results_file, 'wb') as output_file:\n",
    "        pickle.dump(efs_split_regression_results, output_file)\n",
    "\n",
    "else:\n",
    "    predictions=efs_split_regression_results['Testing predictions']\n",
    "    labels=efs_split_regression_results['Testing labels']\n",
    "    best_parameters=efs_split_regression_results['Best parameters']\n",
    "    models=efs_split_regression_results['Tuned models']\n",
    "    race_groups=efs_split_regression_results['Race groups']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_results=helper_funcs.score_predictions(\n",
    "    'Combined',\n",
    "    predictions['Combined'],\n",
    "    labels['Combined']['efs_time'].values,\n",
    "    labels['Combined']['efs'].values,\n",
    "    race_groups['Combined'],\n",
    "    labels['Combined'].index\n",
    ")\n",
    "\n",
    "scoring_results=helper_funcs.score_predictions(\n",
    "    'Split',\n",
    "    np.array(list(predictions['EFS 0']) + list(predictions['EFS 1'])),\n",
    "    pd.concat([labels['EFS 0'], labels['EFS 1']], axis=0)['efs_time'].values,\n",
    "    pd.concat([labels['EFS 0'], labels['EFS 1']], axis=0)['efs'].values,\n",
    "    pd.concat([race_groups['EFS 0'], race_groups['EFS 1']], axis=0),\n",
    "    pd.concat([labels['EFS 0'], labels['EFS 1']], axis=0).index,\n",
    "    results=scoring_results\n",
    ")\n",
    "\n",
    "scoring_results=helper_funcs.score_predictions(\n",
    "    'Labels',\n",
    "    labels['Combined']['efs_time'].values,\n",
    "    labels['Combined']['efs_time'].values,\n",
    "    labels['Combined']['efs'].values,\n",
    "    race_groups['Combined'],\n",
    "    labels['Combined'].index,\n",
    "    results=scoring_results\n",
    ")\n",
    "\n",
    "scoring_results_df=pd.DataFrame(scoring_results)\n",
    "print(f'\\n{scoring_results_df.head()}')\n",
    "\n",
    "fig, axs=plt.subplots(1,2, figsize=(9,4))\n",
    "axs=axs.flatten()\n",
    "\n",
    "axs[0].set_title(f'Winning model combined testing data')\n",
    "axs[0].scatter(labels['Combined']['efs_time'], predictions['Combined'], color='black', s=0.2)\n",
    "axs[0].set_xlabel('True EFS time')\n",
    "axs[0].set_ylabel('Predicted EFS time')\n",
    "\n",
    "axs[1].set_title(f'Winning models split testing data')\n",
    "axs[1].scatter(labels['EFS 0']['efs_time'], predictions['EFS 0'], s=0.2, color='black')\n",
    "axs[1].scatter(labels['EFS 1']['efs_time'], predictions['EFS 1'], s=0.2, color='firebrick')\n",
    "axs[1].set_xlabel('True EFS time')\n",
    "axs[1].set_ylabel('Predicted EFS time')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering\n",
    "\n",
    "### 3.1. Gradient boosting\n",
    "\n",
    "#### 3.1.1. Dataset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data_tree_clustering == True:\n",
    "\n",
    "    predictions={}\n",
    "    labels={}\n",
    "    best_parameters={}\n",
    "    models={}\n",
    "\n",
    "    # Define cross-validation strategy\n",
    "    cross_validation=ShuffleSplit(n_splits=10, test_size=0.3, random_state=315)\n",
    "\n",
    "    # Loop on the datasets\n",
    "    for dataset, data_file in datasets.items():\n",
    "\n",
    "        # Load the data\n",
    "        data_df=pd.read_parquet(data_file)\n",
    "\n",
    "        # Take log of efs time\n",
    "        data_df['efs_time']=np.log(data_df['efs_time'])\n",
    "\n",
    "        # Preserve unencoded race group\n",
    "        race_group=data_df['race_group']\n",
    "        data_df.drop('race_group', axis=1, inplace=True)\n",
    "\n",
    "        # Train test split\n",
    "        training_df, testing_df=train_test_split(data_df, test_size=0.3, random_state=315)\n",
    "\n",
    "        # Remove the labels\n",
    "        training_labels_df=training_df[['efs', 'efs_time']].copy()\n",
    "        training_features_df=training_df.drop(['efs', 'efs_time'], axis=1)\n",
    "        testing_labels_df=testing_df[['efs', 'efs_time']].copy()\n",
    "        testing_features_df=testing_df.drop(['efs', 'efs_time'], axis=1)\n",
    "\n",
    "        # Instantiate the model\n",
    "        tree_model=HistGradientBoostingClassifier(random_state=315)\n",
    "\n",
    "        # Set-up the search\n",
    "        search=RandomizedSearchCV(\n",
    "            tree_model,\n",
    "            distributions,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            cv=cross_validation,\n",
    "            n_iter=randomsearch_depth,\n",
    "            random_state=315,\n",
    "            return_train_score=True\n",
    "        )\n",
    "\n",
    "        search_results=search.fit(training_features_df, training_labels_df['efs'])\n",
    "        best_parameters[dataset]=search_results.best_params_\n",
    "        print(f'{dataset} winning parameters: {search_results.best_params_}')\n",
    "\n",
    "        # Train classifier with best hyperparameters on complete training set\n",
    "        tree_model=HistGradientBoostingClassifier(**search_results.best_params_, random_state=315)\n",
    "        result=tree_model.fit(training_features_df, training_labels_df['efs'])\n",
    "        models[dataset]=tree_model\n",
    "\n",
    "        # Make testing predictions\n",
    "        testing_predictions=tree_model.predict(testing_features_df)\n",
    "        predictions[dataset]=testing_predictions\n",
    "        labels[dataset]=testing_labels_df['efs']\n",
    "\n",
    "    datasets_clustering_results={'Testing predictions': predictions}\n",
    "    datasets_clustering_results['Testing labels']=labels\n",
    "    datasets_clustering_results['Best parameters']=best_parameters\n",
    "    datasets_clustering_results['Tuned models']=models\n",
    "\n",
    "    with open(datasets_clustering_results_file, 'wb') as output_file:\n",
    "        pickle.dump(datasets_clustering_results, output_file)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load last result\n",
    "    with open(datasets_clustering_results_file, 'rb') as input_file:\n",
    "        datasets_clustering_results=pickle.load(input_file)\n",
    "\n",
    "    predictions=datasets_clustering_results['Testing predictions']\n",
    "    labels=datasets_clustering_results['Testing labels']\n",
    "    best_parameters=datasets_clustering_results['Best parameters']\n",
    "    models=datasets_clustering_results['Tuned models']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,3, figsize=(15,7))\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('Tree classifier performance: hold-out test set')\n",
    "\n",
    "for i, dataset in enumerate(predictions.keys()):\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    cm=confusion_matrix(labels[dataset], predictions[dataset], normalize='true')\n",
    "    cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    _=cm_disp.plot(ax=axs[i])\n",
    "\n",
    "    axs[i].set_title(dataset)\n",
    "    axs[i].set_xlabel('Predicted EFS')\n",
    "    axs[i].set_ylabel('True EFS')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. Regression on clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clustered_regression_test == True:\n",
    "\n",
    "    # Pick the 'best' model and dataset\n",
    "    winner='All ordinal encoded, NAN imputed'\n",
    "    data_df_file=datasets[winner]\n",
    "    model=models[winner]\n",
    "\n",
    "    # Load the data\n",
    "    data_df=pd.read_parquet(data_df_file)\n",
    "\n",
    "    # Take log of efs time\n",
    "    data_df['efs_time']=np.log(data_df['efs_time'])\n",
    "\n",
    "    # Label dataset with EFS predictions\n",
    "    data_df['predicted_efs']=model.predict(data_df.drop(['efs', 'efs_time', 'race_group'], axis=1))\n",
    "\n",
    "    # Split the data by predicted EFS label\n",
    "    efs_zero_df=data_df[data_df['predicted_efs'] == 0].copy()\n",
    "    efs_one_df=data_df[data_df['predicted_efs'] == 1].copy()\n",
    "\n",
    "    regression_test_datasets={\n",
    "        'Combined': data_df,\n",
    "        'Predicted EFS 0': efs_zero_df,\n",
    "        'Predicted EFS 1': efs_one_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clustered_regression_test == True:\n",
    "    \n",
    "    cross_val_results={\n",
    "        'Dataset':[],\n",
    "        'Mean RMSE': [],\n",
    "        'Standard deviation RMSE': []\n",
    "    }\n",
    "\n",
    "    # Define cross-validation strategy\n",
    "    cv=ShuffleSplit(n_splits=10, test_size=0.3, random_state=315)\n",
    "\n",
    "    # Loop on the datasets\n",
    "    for dataset, dataset_df in regression_test_datasets.items():\n",
    "\n",
    "        # Drop race group\n",
    "        training_df=dataset_df.drop('race_group', axis=1, inplace=False)\n",
    "\n",
    "        # Make features and labels\n",
    "        labels_df=training_df[['efs', 'efs_time']].copy()\n",
    "        features_df=training_df.drop(['efs', 'efs_time'], axis=1, inplace=False)\n",
    "\n",
    "        scores=cross_val_score(\n",
    "            HistGradientBoostingRegressor(random_state=315),\n",
    "            features_df,\n",
    "            labels_df['efs_time'],\n",
    "            cv=cv,\n",
    "            scoring='neg_root_mean_squared_error'\n",
    "        )\n",
    "\n",
    "        scores_mean=np.array(abs(scores)).mean()\n",
    "        scores_std=np.array(abs(scores)).std()\n",
    "        cross_val_results['Dataset'].append(dataset)\n",
    "        cross_val_results['Mean RMSE'].append(scores_mean)\n",
    "        cross_val_results['Standard deviation RMSE'].append(scores_std)\n",
    "\n",
    "    cross_val_results_df=pd.DataFrame.from_dict(cross_val_results)\n",
    "    clustered_regression_results={'Cross-validation scores': cross_val_results_df}\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load last result\n",
    "    with open(clustered_regression_results_file, 'rb') as input_file:\n",
    "        clustered_regression_results=pickle.load(input_file)\n",
    "\n",
    "    cross_val_results_df=clustered_regression_results['Cross-validation scores']\n",
    "\n",
    "cross_val_results_df.head(len(cross_val_results_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clustered_regression_test == True:\n",
    "    \n",
    "    predictions={}\n",
    "    labels={}\n",
    "    best_parameters={}\n",
    "    models={}\n",
    "    race_group={}\n",
    "\n",
    "    # Define the cross-validation strategy\n",
    "    cross_validation=ShuffleSplit(n_splits=10, test_size=0.3, random_state=315)\n",
    "\n",
    "    # Loop on the datasets\n",
    "    for dataset, dataset_df in regression_test_datasets.items():\n",
    "\n",
    "        # Train test split\n",
    "        training_df, testing_df=train_test_split(dataset_df, test_size=0.3, random_state=315)\n",
    "\n",
    "        # Save the testing race group and drop from training and testing\n",
    "        race_groups[dataset]=testing_df['race_group']\n",
    "        training_df.drop('race_group', axis=1, inplace=True)\n",
    "        testing_df.drop('race_group', axis=1, inplace=True)\n",
    "\n",
    "        # Remove the labels\n",
    "        training_labels_df=training_df[['efs', 'efs_time']].copy()\n",
    "        training_features_df=training_df.drop(['efs', 'efs_time'], axis=1)\n",
    "        testing_labels_df=testing_df[['efs', 'efs_time']].copy()\n",
    "        testing_features_df=testing_df.drop(['efs', 'efs_time'], axis=1)\n",
    "\n",
    "        # Instantiate the model\n",
    "        tree_model=HistGradientBoostingRegressor(random_state=315)\n",
    "\n",
    "        # Set-up the search\n",
    "        search=RandomizedSearchCV(\n",
    "            tree_model,\n",
    "            distributions,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            cv=cross_validation,\n",
    "            n_iter=randomsearch_depth,\n",
    "            random_state=315,\n",
    "            return_train_score=True\n",
    "        )\n",
    "\n",
    "        search_results=search.fit(training_features_df, training_labels_df['efs_time'])\n",
    "        best_parameters[dataset]=search_results.best_params_\n",
    "        print(f'{dataset} winning parameters: {search_results.best_params_}')\n",
    "\n",
    "        # Train classifier with best hyperparameters on complete training set\n",
    "        tree_model=HistGradientBoostingRegressor(**search_results.best_params_, random_state=315)\n",
    "        result=tree_model.fit(training_features_df, training_labels_df['efs_time'])\n",
    "        models[dataset]=tree_model\n",
    "\n",
    "        # Make testing predictions\n",
    "        testing_predictions=tree_model.predict(testing_features_df)\n",
    "        predictions[dataset]=testing_predictions\n",
    "        labels[dataset]=testing_labels_df\n",
    "\n",
    "    clustered_regression_results['Testing predictions']=predictions\n",
    "    clustered_regression_results['Testing labels']=labels\n",
    "    clustered_regression_results['Best parameters']=best_parameters\n",
    "    clustered_regression_results['Tuned models']=models\n",
    "    clustered_regression_results['Race groups']=race_groups\n",
    "\n",
    "    with open(clustered_regression_results_file, 'wb') as output_file:\n",
    "        pickle.dump(clustered_regression_results, output_file)\n",
    "\n",
    "else:\n",
    "    predictions=clustered_regression_results['Testing predictions']\n",
    "    labels=clustered_regression_results['Testing labels']\n",
    "    best_parameters=clustered_regression_results['Best parameters']\n",
    "    models=clustered_regression_results['Tuned models']\n",
    "    race_groups=clustered_regression_results['Race groups']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4. Regression results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_results=helper_funcs.score_predictions(\n",
    "    'Combined',\n",
    "    predictions['Combined'],\n",
    "    labels['Combined']['efs_time'].values,\n",
    "    labels['Combined']['efs'].values,\n",
    "    race_groups['Combined'],\n",
    "    labels['Combined'].index\n",
    ")\n",
    "\n",
    "scoring_results=helper_funcs.score_predictions(\n",
    "    'Split',\n",
    "    np.array(list(predictions['Predicted EFS 0']) + list(predictions['Predicted EFS 1'])),\n",
    "    pd.concat([labels['Predicted EFS 0'], labels['Predicted EFS 1']], axis=0)['efs_time'].values,\n",
    "    pd.concat([labels['Predicted EFS 0'], labels['Predicted EFS 1']], axis=0)['efs'].values,\n",
    "    pd.concat([race_groups['Predicted EFS 0'], race_groups['Predicted EFS 1']], axis=0),\n",
    "    pd.concat([labels['Predicted EFS 0'], labels['Predicted EFS 1']], axis=0).index,\n",
    "    results=scoring_results\n",
    ")\n",
    "\n",
    "scoring_results=helper_funcs.score_predictions(\n",
    "    'Labels',\n",
    "    labels['Combined']['efs_time'].values,\n",
    "    labels['Combined']['efs_time'].values,\n",
    "    labels['Combined']['efs'].values,\n",
    "    race_groups['Combined'],\n",
    "    labels['Combined'].index,\n",
    "    results=scoring_results\n",
    ")\n",
    "\n",
    "scoring_results_df=pd.DataFrame(scoring_results)\n",
    "print(scoring_results_df.head())\n",
    "\n",
    "fig, axs=plt.subplots(1,2, figsize=(9,4))\n",
    "axs=axs.flatten()\n",
    "\n",
    "axs[0].set_title(f'Winning model combined testing data')\n",
    "axs[0].scatter(labels['Combined']['efs_time'], predictions['Combined'], color='black', s=0.2)\n",
    "axs[0].set_xlabel('True EFS time')\n",
    "axs[0].set_ylabel('Predicted EFS time')\n",
    "\n",
    "axs[1].set_title(f'Winning models split testing data')\n",
    "axs[1].scatter(labels['Predicted EFS 0']['efs_time'], predictions['Predicted EFS 0'], s=0.2, color='black')\n",
    "axs[1].scatter(labels['Predicted EFS 1']['efs_time'], predictions['Predicted EFS 1'], s=0.2, color='firebrick')\n",
    "axs[1].set_xlabel('True EFS time')\n",
    "axs[1].set_ylabel('Predicted EFS time')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "if train_nn == True:\n",
    "\n",
    "    # Set the weight initializer function\n",
    "    initializer=GlorotUniform(seed=315)\n",
    "\n",
    "    # Set-up the L1L2 for the dense layers\n",
    "    regularizer=L1L2(l1=0.001, l2=0.0001)\n",
    "\n",
    "    starting_neurons_list=[32, 64, 128, 256, 512]\n",
    "    max_neurons_list=[512, 1024, 2048]\n",
    "    learning_rates_list=[0.001, 0.0001, 0.00001]\n",
    "\n",
    "    hyperparameter_sets=product(starting_neurons_list, max_neurons_list, learning_rates_list)\n",
    "\n",
    "    nn_results={}\n",
    "\n",
    "    for hyperparameters in hyperparameter_sets:\n",
    "\n",
    "        starting_neurons=hyperparameters[0]\n",
    "        max_neurons=hyperparameters[1]\n",
    "        learning_rate=hyperparameters[2]\n",
    "        print(f'Starting neurons: {starting_neurons}, max neurons: {max_neurons}, learning rate: {learning_rate}')\n",
    "\n",
    "        model_layers=[Input(shape=training_features_df.shape[1])]\n",
    "        neurons=starting_neurons\n",
    "\n",
    "        while neurons < max_neurons:\n",
    "            model_layers.append(\n",
    "                Dense(neurons, kernel_regularizer=regularizer, activation='relu', kernel_initializer=initializer)\n",
    "            )\n",
    "\n",
    "            neurons*=2\n",
    "\n",
    "        while neurons >= 2:\n",
    "            model_layers.append(\n",
    "                Dense(neurons, kernel_regularizer=regularizer, activation='relu', kernel_initializer=initializer)\n",
    "            )\n",
    "\n",
    "            neurons/=2\n",
    "\n",
    "        model_layers.append(\n",
    "            Dense(1, activation='sigmoid', kernel_initializer=initializer)\n",
    "        )\n",
    "\n",
    "        nn_model=Sequential(model_layers)\n",
    "\n",
    "        nn_model.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            metrics=[Precision(name='precision'), Recall(name='recall')]\n",
    "        )\n",
    "\n",
    "        # Calculate class weighting\n",
    "        pos_examples=sum(training_labels_df['efs'])\n",
    "        neg_examples=len(training_labels_df['efs']) - pos_examples\n",
    "        neg_class_weight=(1 / neg_examples) * (len(training_labels_df['efs']) / 2.0)\n",
    "        pos_class_weight=(1 / pos_examples) * (len(training_labels_df['efs']) / 2.0)\n",
    "\n",
    "        # Do the training run\n",
    "        training_results=nn_model.fit(\n",
    "            training_features_df,\n",
    "            training_labels_df['efs'],\n",
    "            batch_size=256,\n",
    "            validation_split=0.25,\n",
    "            epochs=nn_epochs,\n",
    "            verbose=False,\n",
    "            class_weight={0: neg_class_weight, 1: pos_class_weight}\n",
    "        )\n",
    "\n",
    "        nn_results[hyperparameters].append(training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyperparameters, results in nn_results.items():\n",
    "    \n",
    "    # Set-up a 1x2 figure for binary cross-entropy, precision & recall\n",
    "    fig, axs=plt.subplots(1,3, figsize=(12,3))\n",
    "\n",
    "    # Add the main title\n",
    "    fig.suptitle(f'Starting neurons: {starting_neurons}, max neurons: {max_neurons}, learning rate: {learning_rate}')\n",
    "\n",
    "    axs[0].set_title('Binary cross-entropy')\n",
    "    axs[0].plot(training_results.history['loss'], label='Training')\n",
    "    axs[0].plot(training_results.history['val_loss'], label='Validation')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Binary cross-entropy')\n",
    "    axs[0].legend(loc='upper left')\n",
    "\n",
    "    axs[1].set_title('Precision')\n",
    "    axs[1].plot(training_results.history['precision'])\n",
    "    axs[1].plot(training_results.history['val_precision'])\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Precision')\n",
    "\n",
    "    axs[2].set_title('Recall')\n",
    "    axs[2].plot(training_results.history['recall'])\n",
    "    axs[2].plot(training_results.history['val_recall'])\n",
    "    axs[2].set_xlabel('Epoch')\n",
    "    axs[2].set_ylabel('Recall')\n",
    "\n",
    "    # Show the plot\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_nn == True:\n",
    "    \n",
    "    predictions=nn_model.predict(testing_features_df)\n",
    "    calls=list(np.where(np.array(predictions) > 0.5, 1, 0))\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision=precision_score(testing_labels_df['efs'], calls)\n",
    "    recall=recall_score(testing_labels_df['efs'], calls)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    cm=confusion_matrix(testing_labels_df['efs'], calls, normalize='true')\n",
    "    cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm)#, display_labels=['0', 'EFS 1'])\n",
    "    _=cm_disp.plot()\n",
    "\n",
    "    plt.title('Test set gradient boosting classifier performance')\n",
    "    plt.xlabel('Predicted EFS')\n",
    "    plt.ylabel('True EFS')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Precision: {precision:.3f}')\n",
    "    print(f'Recall: {recall:.3f}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

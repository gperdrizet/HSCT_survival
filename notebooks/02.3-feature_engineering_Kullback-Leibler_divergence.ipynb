{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSCT survival: Kullback-Leibler divergence\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import configuration as config\n",
    "\n",
    "notebook_num='02.3'\n",
    "gpu=0\n",
    "\n",
    "# Data files\n",
    "datasets_file=f'{config.PROCESSED_DATA}/01.2-dataset_definitions.pkl'\n",
    "coxph_survival_file=f'{config.PROCESSED_DATA}/02.1-coxPH_survival.pkl'\n",
    "weibullaft_survival_file=f'{config.PROCESSED_DATA}/02.2-weibullAFT_survival.pkl'\n",
    "kld_features_file=f'{config.PROCESSED_DATA}/{notebook_num}-kld_survival.pkl'\n",
    "kld_models_file=f'{config.MODELS_PATH}/{notebook_num}-kld_models.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset metadata\n",
    "with open(datasets_file, 'rb') as input_file:\n",
    "    datasets=pickle.load(input_file)\n",
    "\n",
    "# Load one of the datasets\n",
    "with open(datasets['Continuous target encoded'], 'rb') as input_file:\n",
    "    data_dict=pickle.load(input_file)\n",
    "\n",
    "print('Data dictionary contains:\\n')\n",
    "for key, value in data_dict.items():\n",
    "    print(f' {key}: {type(value)}')\n",
    "\n",
    "# Load Cox Proportional Hazard model features\n",
    "with open(coxph_survival_file, 'rb') as input_file:\n",
    "    coxph_features=pickle.load(input_file)\n",
    "\n",
    "print('\\nCox PH features:\\n')\n",
    "for key, value in coxph_features.items():\n",
    "    print(f' {key}: {type(value)}')\n",
    "\n",
    "# Load Weibull Accelerated Failure Time model features\n",
    "with open(weibullaft_survival_file, 'rb') as input_file:\n",
    "    weibullaft_features=pickle.load(input_file)\n",
    "\n",
    "print('\\nWeibull AFT features:\\n')\n",
    "for key, value in weibullaft_features.items():\n",
    "    print(f' {key}: {type(value)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features=['CoxPH survival','CoxPH partial hazard','WeibullAFT survival','WeibullAFT expectation']\n",
    "\n",
    "training_df=data_dict['Training labels']\n",
    "training_df['CoxPH survival']=coxph_features['Training survival']\n",
    "training_df['CoxPH partial hazard']=np.log(coxph_features['Training partial hazard'])\n",
    "training_df['WeibullAFT survival']=weibullaft_features['Training survival']\n",
    "training_df['WeibullAFT expectation']=np.log(weibullaft_features['Training expectation'])\n",
    "\n",
    "testing_df=data_dict['Testing labels']\n",
    "testing_df['CoxPH survival']=coxph_features['Testing survival']\n",
    "testing_df['CoxPH partial hazard']=np.log(coxph_features['Testing partial hazard'])\n",
    "testing_df['WeibullAFT survival']=weibullaft_features['Testing survival']\n",
    "testing_df['WeibullAFT expectation']=np.log(weibullaft_features['Testing expectation'])\n",
    "\n",
    "# scaler=StandardScaler()\n",
    "# scaler.fit(training_df[input_features])\n",
    "# training_df[input_features]=scaler.transform(training_df[input_features])\n",
    "# testing_df[input_features]=scaler.transform(testing_df[input_features])\n",
    "\n",
    "efs_one=training_df[training_df['efs'] == 0]\n",
    "efs_zero=training_df[training_df['efs'] == 1]\n",
    "\n",
    "training_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,2, figsize=(8,4))\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('Feature distributions')\n",
    "\n",
    "for i, feature in enumerate(input_features):\n",
    "    axs[i].hist(efs_one[feature], bins=30, color='firebrick', alpha=0.5, label='EFS 1')\n",
    "    axs[i].hist(efs_zero[feature], bins=30, color='black', alpha=0.5, label='EFS 0')\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].legend(loc='best')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kernel density estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdes={}\n",
    "\n",
    "for feature in input_features:\n",
    "\n",
    "    feature_min=min(training_df[feature])\n",
    "    feature_max=max(training_df[feature])\n",
    "    feature_range=feature_max-feature_min\n",
    "    padding=feature_range*0.05\n",
    "    x=np.linspace(feature_min-padding, feature_max+padding)\n",
    "\n",
    "    efs_zero_kde=gaussian_kde(\n",
    "        efs_zero[feature].values, \n",
    "        bw_method='silverman'\n",
    "    )\n",
    "\n",
    "    efs_one_kde=gaussian_kde(\n",
    "        efs_one[feature].values, \n",
    "        bw_method='silverman'\n",
    "    )\n",
    "\n",
    "    kdes[feature]={\n",
    "        'x': x,\n",
    "        'EFS one': efs_one_kde(x),\n",
    "        'EFS zero': efs_zero_kde(x)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,2, figsize=(8,4))\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('Feature kernel density estimates')\n",
    "\n",
    "for i, feature in enumerate(input_features):\n",
    "    axs[i].hist(efs_one[feature], bins=30, density=True, color='firebrick', alpha=0.5, label='EFS 1')\n",
    "    axs[i].hist(efs_zero[feature], bins=30, density=True, color='black', alpha=0.5, label='EFS 0')\n",
    "    axs[i].plot(kdes[feature]['x'],kdes[feature]['EFS one'], color='firebrick')\n",
    "    axs[i].plot(kdes[feature]['x'],kdes[feature]['EFS zero'], color='black')\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].legend(loc='best')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kullback-Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in input_features:\n",
    "\n",
    "    # Convert inputs to numpy\n",
    "    p=np.asarray(kdes[feature]['EFS one'])\n",
    "    q=np.asarray(kdes[feature]['EFS zero'])\n",
    "\n",
    "    # Set handling for overflows/underflows - just ignore. We will handle infinite\n",
    "    # or nan values later by just filtering them out.\n",
    "    with np.errstate(over='ignore', under='ignore', divide='ignore', invalid='ignore'):\n",
    "        kld_values=p * np.log2(p/q)\n",
    "\n",
    "    kdes[feature]['KLD']=kld_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,2, figsize=(8,6))\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('Feature Kullback-Leibler divergence')\n",
    "\n",
    "for i, feature in enumerate(input_features):\n",
    "    axs[i].plot(kdes[feature]['x'],kdes[feature]['EFS one'], linestyle='dashed', color='firebrick', label='EFS 1')\n",
    "    axs[i].plot(kdes[feature]['x'],kdes[feature]['EFS zero'], linestyle='dashed', color='black', label='EFS 0')\n",
    "    axs[i].plot(kdes[feature]['x'],kdes[feature]['KLD'], color='orange', label='KLD')\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].legend(loc='best')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. KLD kernel density estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in input_features:\n",
    "\n",
    "    # Construct new padded x range\n",
    "    x_min=min(kdes[feature]['x'])\n",
    "    x_max=max(kdes[feature]['x'])\n",
    "    x_range=x_max-x_min\n",
    "    padding=x_range*0.05\n",
    "    x=np.linspace(x_min-padding, x_max+padding)\n",
    "\n",
    "    # Shift the kld values so that they are non-negative\n",
    "    kld_abs=abs(min(kdes[feature]['KLD']))\n",
    "    kld=kdes[feature]['KLD'] + kld_abs\n",
    "\n",
    "    # Scale the values so when we convert to integer we get good\n",
    "    # resolution, e.g. we don't want to collapse 2.1, 2.2, 2.3 etc.,\n",
    "    # to 2. Instead, 2100.0, 2200.0, 2300.0 become 2100, 2200, 2300 etc.\n",
    "    kld=kld * 100000\n",
    "\n",
    "    # Convert to integer\n",
    "    kld_counts=kld.astype(int)\n",
    "\n",
    "    # Now, construct a list where each value of x appears a number of times\n",
    "    # equal to it's KLD 'count'\n",
    "    kld_scores=[]\n",
    "\n",
    "    for i, _ in enumerate(kld_counts):\n",
    "        kld_scores.extend([x[i]] * kld_counts[i])\n",
    "\n",
    "    kld_kde=gaussian_kde(\n",
    "        kld_scores, \n",
    "        bw_method='silverman'\n",
    "    )\n",
    "\n",
    "    # Clip x back to the original range of the data\n",
    "    feature_min=min(training_df[feature])\n",
    "    feature_max=max(training_df[feature])\n",
    "    feature_range=feature_max-feature_min\n",
    "    padding=feature_range*0.05\n",
    "    x=np.linspace(feature_min-padding, feature_max+padding)\n",
    "\n",
    "    kdes[feature]['x']=x\n",
    "    kdes[feature]['KLD KDE values']=kld_kde(x)\n",
    "    kdes[feature]['KLD KDE']=kld_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,2, figsize=(8,6))\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('Feature Kullback-Leibler divergence')\n",
    "\n",
    "for i, feature in enumerate(input_features):\n",
    "    axs[i].plot(kdes[feature]['x'],kdes[feature]['EFS one'], linestyle='dashed', color='firebrick', label='EFS 1')\n",
    "    axs[i].plot(kdes[feature]['x'],kdes[feature]['EFS zero'], linestyle='dashed', color='black', label='EFS 0')\n",
    "    axs[i].plot(kdes[feature]['x'],kdes[feature]['KLD KDE values'], color='Green', label='KLD KDE')\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].legend(loc='best')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Score training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in input_features:\n",
    "    kld_scores=kdes[feature]['KLD KDE'](training_df[feature])\n",
    "    training_df[f'{feature}_KLD']=kld_scores\n",
    "\n",
    "    kld_scores=kdes[feature]['KLD KDE'](testing_df[feature])\n",
    "    testing_df[f'{feature}_KLD']=kld_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,2, figsize=(8,6))\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('Feature Kullback-Leibler divergence scores')\n",
    "\n",
    "for i, feature in enumerate(input_features):\n",
    "    axs[i].scatter(training_df[feature], training_df[f'{feature}_KLD'], s=0.2, color='black')\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].set_ylabel('KLD score')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save KLD features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.head(len(training_df)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kld_features={\n",
    "    'Training CoxPH survival KLD':training_df['CoxPH survival_KLD'].values,\n",
    "    'Training CoxPH partial hazard KLD':training_df['CoxPH partial hazard_KLD'].values,\n",
    "    'Training WeibullAFT survival KLD':training_df['WeibullAFT survival_KLD'].values,\n",
    "    'Training WeibullAFT expectation KLD':training_df['WeibullAFT expectation_KLD'].values,\n",
    "    'Testing CoxPH survival KLD':testing_df['CoxPH survival_KLD'].values,\n",
    "    'Testing CoxPH partial hazard KLD':testing_df['CoxPH partial hazard_KLD'].values,\n",
    "    'Testing WeibullAFT survival KLD':testing_df['WeibullAFT survival_KLD'].values,\n",
    "    'Testing WeibullAFT expectation KLD':testing_df['WeibullAFT expectation_KLD'].values\n",
    "}\n",
    "\n",
    "with open(kld_features_file, 'wb') as output_file:\n",
    "    pickle.dump(kld_features, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save KLD models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models={}\n",
    "\n",
    "for feature in input_features:\n",
    "    models[feature]=kdes[feature]['KLD KDE']\n",
    "\n",
    "with open(kld_models_file, 'wb') as output_file:\n",
    "    pickle.dump(models, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

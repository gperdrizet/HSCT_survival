{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSCT survival: XGBoost models datasets test\n",
    "\n",
    "## 1. Notebook set-up\n",
    "\n",
    "### 1.1. Imports & options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyPI imports\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config\n",
    "import functions.helper as helper_funcs\n",
    "\n",
    "notebook_num='01.3'\n",
    "gpu=0\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "os.environ['OMP_NUM_THREADS']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "regression_test=True\n",
    "classification_test=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition file\n",
    "datasets_file=f'{config.PROCESSED_DATA}/01.2-dataset_definitions.pkl'\n",
    "\n",
    "# Save the dataset metadata\n",
    "with open(datasets_file, 'rb') as input_file:\n",
    "    datasets=pickle.load(input_file)\n",
    "\n",
    "# Dataset testing results\n",
    "regression_test_results_file=f'{config.RESULTS}/{notebook_num}-regression_test_results.pkl'\n",
    "regression_training_performance_plots=f'{config.PLOTS}/{notebook_num}-regression_training_performance.jpg'\n",
    "regression_test_performance_plots=f'{config.PLOTS}/{notebook_num}-regression_test_performance.jpg'\n",
    "regression_test_residuals_plots=f'{config.PLOTS}/{notebook_num}-regression_test_residuals.jpg'\n",
    "classification_test_results_file=f'{config.RESULTS}/{notebook_num}-classification_test_results.pkl'\n",
    "classification_test_performance_plots=f'{config.PLOTS}/{notebook_num}-classification_test_performance.jpg'\n",
    "classification_test_probability_plots=f'{config.PLOTS}/{notebook_num}-classification_test_probability.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost regression model\n",
    "\n",
    "### 2.1. Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if regression_test == True:\n",
    "\n",
    "    # Define the hyperparameter search space\n",
    "    search_space={\n",
    "        'objective': ['reg:squarederror'],\n",
    "        'eval_metric': ['rmse'],\n",
    "        'gpu_id': [gpu],\n",
    "        'tree_method': ['gpu_hist'],\n",
    "        'seed': [315],\n",
    "        'learning_rate': [0.005,0.01],\n",
    "        'max_depth': [4,6,8,10],\n",
    "        'gamma': [0.01,0.02,0.04,0.06],\n",
    "        'subsample': [0.3,0.4,0.5,0.6]\n",
    "    }\n",
    "\n",
    "    regression_predictions={\n",
    "        'Training':{},\n",
    "        'Testing':{}\n",
    "    }\n",
    "\n",
    "    # Loop on the datasets\n",
    "    for dataset, data_file in datasets.items():\n",
    "\n",
    "        # Load the data\n",
    "        with open(data_file, 'rb') as input_file:\n",
    "            data_dict=pickle.load(input_file)\n",
    "\n",
    "        # Prepare data for training\n",
    "        training_labels_df=data_dict['Training labels']\n",
    "        training_features_df=data_dict['Training features']\n",
    "        testing_labels_df=data_dict['Testing labels']\n",
    "        testing_features_df=data_dict['Testing features']\n",
    "\n",
    "        training_labels_df['efs_time']=np.log(training_labels_df['efs_time'])\n",
    "        testing_labels_df['efs_time']=np.log(testing_labels_df['efs_time'])\n",
    "\n",
    "        dtraining=xgb.DMatrix(training_features_df, label=training_labels_df['efs_time'])\n",
    "        dtesting=xgb.DMatrix(testing_features_df, label=testing_labels_df['efs_time'])\n",
    "\n",
    "        results_df=helper_funcs.xgb_hyperparameter_search(\n",
    "            search_space,\n",
    "            training_features_df,\n",
    "            training_labels_df['efs_time']\n",
    "        )\n",
    "\n",
    "        winning_hyperparameters=dict(results_df.iloc[-1]['Hyperparameters'])\n",
    "        \n",
    "        print(f'\\n{dataset}:')\n",
    "        for parameter, value in winning_hyperparameters.items():\n",
    "            print(f' {parameter}: {value}')\n",
    "\n",
    "        # Train classifier with best hyperparameters on complete training set\n",
    "        tree_model=xgb.train(\n",
    "            winning_hyperparameters,\n",
    "            dtraining,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dtraining, 'training')],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=0\n",
    "        )\n",
    "\n",
    "        # Make training predictions\n",
    "        predictions=tree_model.predict(dtraining)\n",
    "        regression_predictions['Training'][dataset]=predictions\n",
    "\n",
    "        # Make testing predictions\n",
    "        predictions=tree_model.predict(dtesting)\n",
    "        regression_predictions['Testing'][dataset]=predictions\n",
    "\n",
    "    with open(regression_test_results_file, 'wb') as output_file:\n",
    "        pickle.dump(regression_predictions, output_file)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load last result\n",
    "    with open(regression_test_results_file, 'rb') as input_file:\n",
    "        regression_predictions=pickle.load(input_file)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Nominal one-hot/ordinal encoded, NANs encoded:\n",
    " objective: reg:squarederror\n",
    " eval_metric: rmse\n",
    " gpu_id: 0\n",
    " tree_method: gpu_hist\n",
    " seed: 315\n",
    " learning_rate: 0.01\n",
    " max_depth: 6\n",
    " gamma: 0.02\n",
    " subsample: 0.5\n",
    "\n",
    "Nominal one-hot/ordinal encoded, NANs imputed:\n",
    " objective: reg:squarederror\n",
    " eval_metric: rmse\n",
    " gpu_id: 0\n",
    " tree_method: gpu_hist\n",
    " seed: 315\n",
    " learning_rate: 0.01\n",
    " max_depth: 6\n",
    " gamma: 0.02\n",
    " subsample: 0.5\n",
    "\n",
    "All ordinal encoded, NAN encoded:\n",
    " objective: reg:squarederror\n",
    " eval_metric: rmse\n",
    " gpu_id: 0\n",
    " tree_method: gpu_hist\n",
    " seed: 315\n",
    " learning_rate: 0.01\n",
    " max_depth: 6\n",
    " gamma: 0.01\n",
    " subsample: 0.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Results\n",
    "\n",
    "#### 2.2.1. Scores\n",
    "\n",
    "##### 2.2.1.1. Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_results={\n",
    "    'Model': [],\n",
    "    'RMSE': [],\n",
    "    'C-index': [],\n",
    "    'Stratified C-index': []\n",
    "}\n",
    "\n",
    "for dataset in regression_predictions['Training'].keys():\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    scoring_results=helper_funcs.score_predictions(\n",
    "        dataset,\n",
    "        regression_predictions['Training'][dataset],\n",
    "        np.log(data_dict['Training labels']['efs_time'].values),\n",
    "        data_dict['Training labels']['efs'].values,\n",
    "        data_dict['Training race group'],\n",
    "        data_dict['Training IDs'],\n",
    "        results=scoring_results\n",
    "    )\n",
    "\n",
    "scoring_results_df=pd.DataFrame(scoring_results)\n",
    "scoring_results_df.head(len(scoring_results_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1.2. Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_results={\n",
    "    'Model': [],\n",
    "    'RMSE': [],\n",
    "    'C-index': [],\n",
    "    'Stratified C-index': []\n",
    "}\n",
    "\n",
    "for dataset in regression_predictions['Testing'].keys():\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    scoring_results=helper_funcs.score_predictions(\n",
    "        dataset,\n",
    "        regression_predictions['Testing'][dataset],\n",
    "        np.log(data_dict['Testing labels']['efs_time'].values),\n",
    "        data_dict['Testing labels']['efs'].values,\n",
    "        data_dict['Testing race group'],\n",
    "        data_dict['Testing IDs'],\n",
    "        results=scoring_results\n",
    "    )\n",
    "\n",
    "scoring_results_df=pd.DataFrame(scoring_results)\n",
    "scoring_results_df.head(len(scoring_results_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Prediction plots\n",
    "\n",
    "##### 2.2.2.1. Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(3,3, figsize=(8,8), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('XGBoost regression performance: training set')\n",
    "\n",
    "for i, dataset in enumerate(regression_predictions['Training'].keys()):\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    axs[i].set_title(dataset.replace(', ', '\\n').replace('/', '\\n'))\n",
    "    axs[i].scatter(\n",
    "        np.log(data_dict['Training labels']['efs_time'].values),\n",
    "        regression_predictions['Training'][dataset],\n",
    "        color='black',\n",
    "        s=0.2\n",
    "    )\n",
    "    axs[i].set_xlabel('True EFS time')\n",
    "    axs[i].set_ylabel('Predicted EFS time')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(regression_training_performance_plots, dpi=300, bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.2. Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(3,3, figsize=(8,8), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('XGBoost regression performance: hold-out test set')\n",
    "\n",
    "for i, dataset in enumerate(regression_predictions['Testing'].keys()):\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    axs[i].set_title(dataset.replace(', ', '\\n').replace('/', '\\n'))\n",
    "    axs[i].scatter(\n",
    "        np.log(data_dict['Testing labels']['efs_time'].values),\n",
    "        regression_predictions['Testing'][dataset],\n",
    "        color='black',\n",
    "        s=0.2\n",
    "    )\n",
    "    axs[i].set_xlabel('True EFS time')\n",
    "    axs[i].set_ylabel('Predicted EFS time')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(regression_test_performance_plots, dpi=300, bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Residual plots\n",
    "\n",
    "##### 2.2.3.1. Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(3,3, figsize=(8,8), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('XGBoost regression fit residuals: training set')\n",
    "\n",
    "for i, dataset in enumerate(regression_predictions['Training'].keys()):\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    axs[i].set_title(dataset.replace(', ', '\\n').replace('/', '\\n'))\n",
    "    axs[i].scatter(\n",
    "        regression_predictions['Training'][dataset],\n",
    "        np.log(data_dict['Training labels']['efs_time'].values) - regression_predictions['Training'][dataset],\n",
    "        color='black',\n",
    "        s=0.2\n",
    "    )\n",
    "    axs[i].set_xlabel('Predicted EFS time')\n",
    "    axs[i].set_ylabel('EFS time residual')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(regression_test_residuals_plots, dpi=300, bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3.2. Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(3,3, figsize=(8,8), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('XGBoost regression fit residuals: hold-out test set')\n",
    "\n",
    "for i, dataset in enumerate(regression_predictions['Testing'].keys()):\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    axs[i].set_title(dataset.replace(', ', '\\n').replace('/', '\\n'))\n",
    "    axs[i].scatter(\n",
    "        regression_predictions['Testing'][dataset],\n",
    "        np.log(data_dict['Testing labels']['efs_time'].values) - regression_predictions['Testing'][dataset],\n",
    "        color='black',\n",
    "        s=0.2\n",
    "    )\n",
    "    axs[i].set_xlabel('Predicted EFS time')\n",
    "    axs[i].set_ylabel('EFS time residual')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(regression_test_residuals_plots, dpi=300, bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoost classification model\n",
    "\n",
    "### 3.1. Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if classification_test == True:\n",
    "\n",
    "    classification_predictions={\n",
    "        'Training':{},\n",
    "        'Testing':{}\n",
    "    }\n",
    "\n",
    "    # Loop on the datasets\n",
    "    for dataset, data_file in datasets.items():\n",
    "\n",
    "        # Load the data\n",
    "        with open(data_file, 'rb') as input_file:\n",
    "            data_dict=pickle.load(input_file)\n",
    "\n",
    "        # Prepare data for training\n",
    "        training_labels_df=data_dict['Training labels']\n",
    "        training_features_df=data_dict['Training features']\n",
    "        testing_labels_df=data_dict['Testing labels']\n",
    "        testing_features_df=data_dict['Testing features']\n",
    "\n",
    "        dtraining=xgb.DMatrix(training_features_df, label=training_labels_df['efs'])\n",
    "        dtesting=xgb.DMatrix(testing_features_df, label=testing_labels_df['efs'])\n",
    "\n",
    "        # Calculated class weighting\n",
    "        class_weight=(len(training_labels_df) - sum(training_labels_df['efs'])) / sum(training_labels_df['efs'])\n",
    "\n",
    "        # Define the hyperparameter search space\n",
    "        search_space={\n",
    "            'objective': ['binary:logistic'],\n",
    "            'eval_metric': ['logloss'],\n",
    "            'gpu_id': [gpu],\n",
    "            'tree_method': ['gpu_hist'],\n",
    "            'scale_pos_weight': [class_weight],\n",
    "            'seed': [315],\n",
    "            'learning_rate': [0.005,0.01],\n",
    "            'max_depth': [4,6,8,10],\n",
    "            'gamma': [0.01,0.02,0.04,0.06],\n",
    "            'subsample': [0.3,0.4,0.5,0.6]\n",
    "        }\n",
    "\n",
    "        results_df=helper_funcs.xgb_hyperparameter_search(\n",
    "            search_space,\n",
    "            training_features_df,\n",
    "            training_labels_df['efs']\n",
    "        )\n",
    "\n",
    "        winning_hyperparameters=dict(results_df.iloc[-1]['Hyperparameters'])\n",
    "        \n",
    "        print(f'\\n{dataset}:')\n",
    "        for parameter, value in winning_hyperparameters.items():\n",
    "            print(f' {parameter}: {value}')\n",
    "\n",
    "        # Train classifier with best hyperparameters on complete training set\n",
    "        tree_model=xgb.train(\n",
    "            winning_hyperparameters,\n",
    "            dtraining,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dtraining, 'training')],\n",
    "            early_stopping_rounds=10,\n",
    "            verbose_eval=0\n",
    "        )\n",
    "\n",
    "        # Make training predictions\n",
    "        predictions=tree_model.predict(dtraining)\n",
    "        classification_predictions['Training'][dataset]=predictions\n",
    "\n",
    "        # Make testing predictions\n",
    "        predictions=tree_model.predict(dtesting)\n",
    "        classification_predictions['Testing'][dataset]=predictions\n",
    "\n",
    "    with open(classification_test_results_file, 'wb') as output_file:\n",
    "        pickle.dump(classification_predictions, output_file)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load last result\n",
    "    with open(classification_test_results_file, 'rb') as input_file:\n",
    "        classification_predictions=pickle.load(input_file)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Results\n",
    "\n",
    "#### 3.2.1. Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(3,3, figsize=(8,8), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('XGBoost classifier performance: hold-out test set')\n",
    "\n",
    "for i, dataset in enumerate(classification_predictions['Testing'].keys()):\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    # Make calls with threshold\n",
    "    calls=np.where(classification_predictions['Testing'][dataset] < 0.5, 0, 1)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    cm=confusion_matrix(data_dict['Testing labels']['efs'], calls, normalize='true')\n",
    "    cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    _=cm_disp.plot(ax=axs[i])\n",
    "\n",
    "    axs[i].set_title(dataset.replace(', ', '\\n').replace('/', '\\n'))\n",
    "    axs[i].set_xlabel('Predicted EFS')\n",
    "    axs[i].set_ylabel('True EFS')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(classification_test_performance_plots, dpi=300, bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(3,3, figsize=(8,8), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('XGBoost classifier probabilities: hold-out test set')\n",
    "\n",
    "for i, dataset in enumerate(classification_predictions['Testing'].keys()):\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    class_df=pd.DataFrame.from_dict({\n",
    "        'EFS': data_dict['Testing labels']['efs'].values,\n",
    "        'EFS probability': classification_predictions['Testing'][dataset]\n",
    "    })\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    sns.histplot(class_df, x='EFS probability', hue='EFS', ax=axs[i])\n",
    "\n",
    "    axs[i].set_title(dataset.replace(', ', '\\n').replace('/', '\\n'))\n",
    "    axs[i].set_xlabel('Predicted EFS')\n",
    "    axs[i].set_ylabel('True EFS')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(classification_test_probability_plots, dpi=300, bbox_inches='tight')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

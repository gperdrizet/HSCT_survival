{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSCT survival: data cleaning and encoding\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import configuration as config\n",
    "\n",
    "# Input data files\n",
    "dictionary_file=f'{config.DATA_PATH}/raw/data_dictionary.csv'\n",
    "training_file=f'{config.DATA_PATH}/raw/train.csv'\n",
    "testing_file=f'{config.DATA_PATH}/raw/test.csv'\n",
    "\n",
    "# Training data with no manipulations of any kind, except ID column set as index\n",
    "# and missing string placeholders converted to nan\n",
    "training_features_df_file=f'{config.DATA_PATH}/processed/02.1-training_features_df.parquet'\n",
    "training_labels_df_file=f'{config.DATA_PATH}/processed/02.1-training_labels_df.parquet'\n",
    "\n",
    "# Dummy encoded data with no other manipulations\n",
    "encoded_training_features_df_file=f'{config.DATA_PATH}/processed/02.1-encoded_training_features_df.parquet'\n",
    "encoded_training_labels_df_file=f'{config.DATA_PATH}/processed/02.1-encoded_training_labels_df.parquet'\n",
    "\n",
    "# Dummy encoded data with NAN rows dropped\n",
    "encoded_cleaned_training_features_df_file=f'{config.DATA_PATH}/processed/02.1-encoded_cleaned_training_features_df.parquet'\n",
    "encoded_cleaned_training_labels_df_file=f'{config.DATA_PATH}/processed/02.1-encoded_cleaned_training_labels_df.parquet'\n",
    "\n",
    "# Dummy encoded data with with NAN values encoded as missing for \n",
    "# true categorical features and KNN imputed for numerical features\n",
    "encoded_missing_imputed_training_features_df_file=f'{config.DATA_PATH}/processed/02.1-encoded_missing_imputed_training_features_df.parquet'\n",
    "encoded_missing_imputed_training_labels_df_file=f'{config.DATA_PATH}/processed/02.1-encoded_missing_imputed_training_labels_df.parquet'\n",
    "\n",
    "# Dummy encoded data with NANs filled in by KNN imputation for all features\n",
    "encoded_all_imputed_training_features_df_file=f'{config.DATA_PATH}/processed/02.1-encoded_all_imputed_training_features_df.parquet'\n",
    "encoded_all_imputed_training_labels_df_file=f'{config.DATA_PATH}/processed/02.1-encoded_all_imputed_training_labels_df.parquet'\n",
    "\n",
    "# Feature info files\n",
    "feature_types_dict_file=f'{config.DATA_PATH}/processed/01.1-feature_type_dict.pkl'\n",
    "feature_value_translation_dicts_file=f'{config.DATA_PATH}/processed/01.1-feature_value_translation_dicts.pkl'\n",
    "nan_placeholders_dict_file=f'{config.DATA_PATH}/processed/01.1-nan_placeholders_list.pkl'\n",
    "\n",
    "# Model files\n",
    "knn_imputer_file=f'{config.MODELS_PATH}/02.1-KNN_imputer.pkl'\n",
    "one_hot_encoder_all_features_raw_file=f'{config.MODELS_PATH}/02.1-one_hot_encoder_all_features_raw.pkl'\n",
    "standard_scaler_file=f'{config.MODELS_PATH}/02.1-standard_scaler.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (28800, 59)\n",
      "Training features: (28800, 57)\n",
      "Training labels: (28800, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "dictionary_df=pd.read_csv(dictionary_file)\n",
    "training_df=pd.read_csv(training_file)\n",
    "\n",
    "# Set the ID column as the index\n",
    "training_df.set_index('ID', drop=True, inplace=True)\n",
    "\n",
    "# Remove the labels from the training dataframe\n",
    "training_labels_df=training_df[['efs', 'efs_time']].copy()\n",
    "training_features_df=training_df.drop(['efs', 'efs_time'], axis=1)\n",
    "\n",
    "# Save\n",
    "training_features_df.to_parquet(training_features_df_file)\n",
    "training_labels_df.to_parquet(training_labels_df_file)\n",
    "\n",
    "print(f'Training data: {training_df.shape}')\n",
    "print(f'Training features: {training_features_df.shape}')\n",
    "print(f'Training labels: {training_labels_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encode features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded training features: (28800, 43336)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28800 entries, 0 to 28799\n",
      "Columns: 43336 entries, dri_score_High - TED AML case <missing cytogenetics to hla_low_res_10_nan\n",
      "dtypes: float64(43336)\n",
      "memory usage: 9.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Encode the features\n",
    "encoder=OneHotEncoder(drop='first', sparse_output=False)\n",
    "encoded_training_data=encoder.fit_transform(training_features_df)\n",
    "\n",
    "# Save the one-hot encoder for later\n",
    "with open(one_hot_encoder_all_features_raw_file, 'wb') as output_file:\n",
    "    pickle.dump(encoder, output_file)\n",
    "\n",
    "# Rebuild the dataframe\n",
    "feature_names=encoder.get_feature_names_out()\n",
    "\n",
    "encoded_training_features_df=pd.DataFrame(\n",
    "    encoded_training_data,\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "print(f'Encoded training features: {encoded_training_features_df.shape}')\n",
    "encoded_training_features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testing_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m training_categorical_df\u001b[38;5;241m=\u001b[39mtraining_df[categorical_feature_names]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     11\u001b[0m training_numerical_df\u001b[38;5;241m=\u001b[39mtraining_df[numerical_feature_names]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 12\u001b[0m testing_categorical_df\u001b[38;5;241m=\u001b[39m\u001b[43mtesting_df\u001b[49m[categorical_feature_names]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     13\u001b[0m testing_numerical_df\u001b[38;5;241m=\u001b[39mtesting_df[numerical_feature_names]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining numerical features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_numerical_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testing_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Get lists of categorical and numerical column names\n",
    "categorical_feature_names=dictionary_df['variable'][dictionary_df['type'] == 'Categorical']\n",
    "numerical_feature_names=dictionary_df['variable'][dictionary_df['type'] == 'Numerical']\n",
    "\n",
    "# Remove the feature column from the column names lists\n",
    "categorical_feature_names=categorical_feature_names[categorical_feature_names != 'efs']\n",
    "numerical_feature_names=numerical_feature_names[numerical_feature_names != 'efs_time']\n",
    "\n",
    "# Split the training and testing dataframes\n",
    "training_categorical_df=training_df[categorical_feature_names].copy()\n",
    "training_numerical_df=training_df[numerical_feature_names].copy()\n",
    "testing_categorical_df=testing_df[categorical_feature_names].copy()\n",
    "testing_numerical_df=testing_df[numerical_feature_names].copy()\n",
    "\n",
    "print(f'Training numerical features: {training_numerical_df.shape}')\n",
    "print(f'Training categorical features: {training_categorical_df.shape}')\n",
    "print(f'Testing numerical features: {testing_numerical_df.shape}')\n",
    "print(f'Testing categorical features: {testing_categorical_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handle missing data\n",
    "\n",
    "### 3.1. Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categorical_df.fillna('Missing', inplace=True)\n",
    "testing_categorical_df.fillna('Missing', inplace=True)\n",
    "\n",
    "print(f'Training categorical features: {training_categorical_df.shape}')\n",
    "print(f'Testing categorical features: {testing_categorical_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use K-nearest neighbor imputation to fill in missing data\n",
    "imputer=KNNImputer(n_neighbors=3, weights='uniform')\n",
    "imputer.fit(training_numerical_df)\n",
    "\n",
    "training_numerical_data=imputer.transform(training_numerical_df)\n",
    "testing_numerical_data=imputer.transform(testing_numerical_df)\n",
    "\n",
    "# Save the imputer for later\n",
    "with open(knn_imputer_file, 'wb') as output_file:\n",
    "    pickle.dump(imputer, output_file)\n",
    "\n",
    "# Re-build dataframes\n",
    "training_numerical_df=pd.DataFrame(\n",
    "    training_numerical_data, \n",
    "    columns=training_numerical_df.columns\n",
    ")\n",
    "\n",
    "testing_numerical_df=pd.DataFrame(\n",
    "    testing_numerical_data,\n",
    "    columns=testing_numerical_df.columns\n",
    ")\n",
    "\n",
    "print(f'Training numerical features: {training_numerical_df.shape}')\n",
    "print(f'Testing numerical features: {testing_numerical_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the features\n",
    "encoder=OneHotEncoder(drop='first', sparse_output=False)\n",
    "encoder.fit(training_categorical_df)\n",
    "\n",
    "training_categorical_data=encoder.transform(training_categorical_df)\n",
    "testing_categorical_data=encoder.transform(testing_categorical_df)\n",
    "\n",
    "# Save the one-hot encoder for later\n",
    "with open(one_hot_encoder_file, 'wb') as output_file:\n",
    "    pickle.dump(encoder, output_file)\n",
    "\n",
    "# Rebuild the dataframes\n",
    "feature_names=encoder.get_feature_names_out()\n",
    "\n",
    "training_categorical_df=pd.DataFrame(\n",
    "    training_categorical_data,\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "testing_categorical_df=pd.DataFrame(\n",
    "    testing_categorical_data,\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "print(f'Training categorical features: {training_categorical_df.shape}')\n",
    "print(f'Testing categorical features: {testing_categorical_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Re-combine numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_df=pd.concat(\n",
    "    [\n",
    "        training_numerical_df.reset_index(drop=True), \n",
    "        training_categorical_df.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "testing_features_df=pd.concat(\n",
    "    [\n",
    "        testing_numerical_df.reset_index(drop=True), \n",
    "        testing_categorical_df.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f'Training features: {training_features_df.shape}')\n",
    "print(f'Testing features: {testing_features_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "scaler.fit(training_features_df)\n",
    "\n",
    "training_data=scaler.transform(training_features_df)\n",
    "testing_data=scaler.transform(testing_features_df)\n",
    "\n",
    "training_features_df=pd.DataFrame(\n",
    "    training_data,\n",
    "    columns=training_features_df.columns\n",
    ")\n",
    "\n",
    "testing_features_df=pd.DataFrame(\n",
    "    testing_data,\n",
    "    columns=testing_features_df.columns\n",
    ")\n",
    "\n",
    "with open(standard_scaler_file, 'wb') as output_file:\n",
    "    pickle.dump(scaler, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transform and scale labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_df['efs_time']=np.log(training_labels_df['efs_time'])\n",
    "\n",
    "scaler=StandardScaler()\n",
    "\n",
    "training_labels_df['efs_time']=scaler.fit_transform(\n",
    "    np.array(training_labels_df['efs_time']).reshape(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if needed\n",
    "Path(f'{config.DATA_PATH}/processed').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add back ID column\n",
    "training_features_df['ID']=training_ids\n",
    "testing_features_df['ID']=testing_ids\n",
    "\n",
    "training_features_df.to_parquet(training_features_df_file, index=False)\n",
    "training_labels_df.to_parquet(training_labels_df_file, index=False)\n",
    "testing_features_df.to_parquet(testing_features_df_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

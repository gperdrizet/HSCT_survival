{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSCT survival: SciKit-learn gradient boosting models datasets test\n",
    "\n",
    "## 1. Notebook set-up\n",
    "\n",
    "### 1.1. Imports & options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyPI imports\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import ShuffleSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config\n",
    "import functions.helper as helper_funcs\n",
    "\n",
    "notebook_num='01.4'\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "os.environ['OMP_NUM_THREADS']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "regression_test=True\n",
    "classification_test=True\n",
    "randomsearch_depth=5000\n",
    "cpus=2\n",
    "\n",
    "# Define the hyperparameter search space for gradient boosting regression\n",
    "distributions={\n",
    "    'learning_rate': stats.uniform(loc=0.0, scale=1.0),\n",
    "    'max_iter': list(range(10, 5000)),\n",
    "    'max_leaf_nodes': list(range(2, 1000)),\n",
    "    'max_depth': list(range(2, 1000)),\n",
    "    'min_samples_leaf': list(range(1, 1000)),\n",
    "    'l2_regularization': stats.uniform(loc=0.0, scale=1.0),\n",
    "    'max_features': stats.uniform(loc=0.1, scale=0.9),\n",
    "    'max_bins': list(range(2, 255)),\n",
    "    'interaction_cst': ['pairwise', 'no_interactions']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition file\n",
    "datasets_file=f'{config.PROCESSED_DATA}/02.1-dataset_definitions.pkl'\n",
    "\n",
    "# Save the dataset metadata\n",
    "with open(datasets_file, 'rb') as input_file:\n",
    "    datasets=pickle.load(input_file)\n",
    "\n",
    "# Dataset testing results\n",
    "regression_test_results_file=f'{config.RESULTS}/{notebook_num}-regression_test_results.pkl'\n",
    "regression_training_performance_plots=f'{config.PLOTS}/{notebook_num}-regression_training_performance.jpg'\n",
    "regression_test_performance_plots=f'{config.PLOTS}/{notebook_num}-regression_test_performance.jpg'\n",
    "regression_test_residuals_plots=f'{config.PLOTS}/{notebook_num}-regression_test_residuals.jpg'\n",
    "classification_test_results_file=f'{config.RESULTS}/{notebook_num}-classification_test_results.pkl'\n",
    "classification_test_performance_plots=f'{config.PLOTS}/{notebook_num}-classification_test_performance.jpg'\n",
    "classification_test_probability_plots=f'{config.PLOTS}/{notebook_num}-classification_test_probability.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SciKit-learn regression model\n",
    "\n",
    "### 2.1. Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if regression_test == True:\n",
    "\n",
    "    regression_predictions={\n",
    "        'Training':{},\n",
    "        'Testing':{}\n",
    "    }\n",
    "\n",
    "    # Define cross-validation strategy\n",
    "    cross_validation=ShuffleSplit(n_splits=10, test_size=0.3, random_state=315)\n",
    "\n",
    "    # Loop on the datasets\n",
    "    for dataset, data_file in datasets.items():\n",
    "\n",
    "        # Load the data\n",
    "        with open(data_file, 'rb') as input_file:\n",
    "            data_dict=pickle.load(input_file)\n",
    "\n",
    "        training_labels_df=data_dict['Training labels']\n",
    "        training_features_df=data_dict['Training features']\n",
    "\n",
    "        # Take log of efs time\n",
    "        training_labels_df['efs_time']=np.log(training_labels_df['efs_time'])\n",
    "\n",
    "        # Instantiate the model\n",
    "        tree_model=HistGradientBoostingRegressor(random_state=315)\n",
    "\n",
    "        # Set-up the search\n",
    "        search=RandomizedSearchCV(\n",
    "            tree_model,\n",
    "            distributions,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=cpus,\n",
    "            cv=cross_validation,\n",
    "            n_iter=randomsearch_depth,\n",
    "            random_state=315,\n",
    "            return_train_score=True\n",
    "        )\n",
    "\n",
    "        search_results=search.fit(training_features_df, training_labels_df['efs'])\n",
    "        \n",
    "        print(f'\\n{dataset}:')\n",
    "        for parameter, value in search_results.best_params_.items():\n",
    "            print(f' {parameter}: {value}')\n",
    "\n",
    "        # Train classifier with best hyperparameters on complete training set\n",
    "        tree_model=HistGradientBoostingRegressor(**search_results.best_params_, random_state=315)\n",
    "        result=tree_model.fit(training_features_df, training_labels_df['efs'])\n",
    "\n",
    "        # Make training predictions\n",
    "        predictions=tree_model.predict(training_features_df)\n",
    "        regression_predictions['Training'][dataset]=predictions\n",
    "\n",
    "        # Make testing predictions\n",
    "        testing_features_df=data_dict['Testing features']\n",
    "        predictions=tree_model.predict(testing_features_df)\n",
    "        regression_predictions['Testing'][dataset]=predictions\n",
    "\n",
    "    with open(regression_test_results_file, 'wb') as output_file:\n",
    "        pickle.dump(regression_predictions, output_file)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load last result\n",
    "    with open(regression_test_results_file, 'rb') as input_file:\n",
    "        regression_predictions=pickle.load(input_file)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Results\n",
    "\n",
    "#### 2.2.1. Scores\n",
    "\n",
    "##### 2.2.1.1. Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_results={\n",
    "    'Model': [],\n",
    "    'RMSE': [],\n",
    "    'C-index': [],\n",
    "    'Stratified C-index': []\n",
    "}\n",
    "\n",
    "for dataset in regression_predictions['Training'].keys():\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    scoring_results=helper_funcs.score_predictions(\n",
    "        dataset,\n",
    "        regression_predictions['Training'][dataset],\n",
    "        np.log(data_dict['Training labels']['efs_time'].values),\n",
    "        data_dict['Training labels']['efs'].values,\n",
    "        data_dict['Training race group'],\n",
    "        data_dict['Training IDs'],\n",
    "        results=scoring_results\n",
    "    )\n",
    "\n",
    "scoring_results_df=pd.DataFrame(scoring_results)\n",
    "scoring_results_df.head(len(scoring_results_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1.2. Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_results={\n",
    "    'Model': [],\n",
    "    'RMSE': [],\n",
    "    'C-index': [],\n",
    "    'Stratified C-index': []\n",
    "}\n",
    "\n",
    "for dataset in regression_predictions['Testing'].keys():\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    scoring_results=helper_funcs.score_predictions(\n",
    "        dataset,\n",
    "        regression_predictions['Testing'][dataset],\n",
    "        np.log(data_dict['Testing labels']['efs_time'].values),\n",
    "        data_dict['Testing labels']['efs'].values,\n",
    "        data_dict['Testing race group'],\n",
    "        data_dict['Testing IDs'],\n",
    "        results=scoring_results\n",
    "    )\n",
    "\n",
    "scoring_results_df=pd.DataFrame(scoring_results)\n",
    "scoring_results_df.head(len(scoring_results_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Prediction plots\n",
    "\n",
    "##### 2.2.2.1. Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,3, figsize=(8,5.5), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('SciKit-learn gradient boosting\\nregression performance: training set')\n",
    "\n",
    "for i, dataset in enumerate(regression_predictions['Training'].keys()):\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    axs[i].set_title(dataset.replace(', ', '\\n').replace('/', '\\n'))\n",
    "    axs[i].scatter(\n",
    "        np.log(data_dict['Training labels']['efs_time'].values),\n",
    "        regression_predictions['Training'][dataset],\n",
    "        color='black',\n",
    "        s=0.2\n",
    "    )\n",
    "    axs[i].set_xlabel('True EFS time')\n",
    "    axs[i].set_ylabel('Predicted EFS time')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(regression_training_performance_plots, dpi=300, bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.2. Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,3, figsize=(8,5.5), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('SciKit-learn gradient boosting\\nregression performance: hold-out test set')\n",
    "\n",
    "for i, dataset in enumerate(regression_predictions['Testing'].keys()):\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    axs[i].set_title(dataset.replace(', ', '\\n').replace('/', '\\n'))\n",
    "    axs[i].scatter(\n",
    "        np.log(data_dict['Testing labels']['efs_time'].values),\n",
    "        regression_predictions['Testing'][dataset],\n",
    "        color='black',\n",
    "        s=0.2\n",
    "    )\n",
    "    axs[i].set_xlabel('True EFS time')\n",
    "    axs[i].set_ylabel('Predicted EFS time')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(regression_test_performance_plots, dpi=300, bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Residual plots\n",
    "\n",
    "##### 2.2.3.1. Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,3, figsize=(8,5.5), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('SciKit-learn gradient boosting\\nregression fit residuals: training set')\n",
    "\n",
    "for i, dataset in enumerate(regression_predictions['Training'].keys()):\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    axs[i].set_title(dataset.replace(', ', '\\n').replace('/', '\\n'))\n",
    "    axs[i].scatter(\n",
    "        regression_predictions['Training'][dataset],\n",
    "        np.log(data_dict['Training labels']['efs_time'].values) - regression_predictions['Training'][dataset],\n",
    "        color='black',\n",
    "        s=0.2\n",
    "    )\n",
    "    axs[i].set_xlabel('Predicted EFS time')\n",
    "    axs[i].set_ylabel('EFS time residual')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(regression_test_residuals_plots, dpi=300, bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3.2. Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,3, figsize=(8,5.5), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('SciKit-learn gradient boosting\\nregression fit residuals: hold-out test set')\n",
    "\n",
    "for i, dataset in enumerate(regression_predictions['Testing'].keys()):\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    axs[i].set_title(dataset.replace(', ', '\\n').replace('/', '\\n'))\n",
    "    axs[i].scatter(\n",
    "        regression_predictions['Testing'][dataset],\n",
    "        np.log(data_dict['Testing labels']['efs_time'].values) - regression_predictions['Testing'][dataset],\n",
    "        color='black',\n",
    "        s=0.2\n",
    "    )\n",
    "    axs[i].set_xlabel('Predicted EFS time')\n",
    "    axs[i].set_ylabel('EFS time residual')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(regression_test_residuals_plots, dpi=300, bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SciKit-learn classification model\n",
    "\n",
    "### 3.1. Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if classification_test == True:\n",
    "\n",
    "    classification_predictions={\n",
    "        'Training':{},\n",
    "        'Testing':{}\n",
    "    }\n",
    "    \n",
    "    classification_models={}\n",
    "\n",
    "    # Define cross-validation strategy\n",
    "    cross_validation=ShuffleSplit(n_splits=10, test_size=0.3, random_state=315)\n",
    "\n",
    "    # Loop on the datasets\n",
    "    for dataset, data_file in datasets.items():\n",
    "\n",
    "        # Load the data\n",
    "        with open(data_file, 'rb') as input_file:\n",
    "            data_dict=pickle.load(input_file)\n",
    "\n",
    "        training_labels_df=data_dict['Training labels']\n",
    "        training_features_df=data_dict['Training features']\n",
    "\n",
    "        # Instantiate the model\n",
    "        classification_model=HistGradientBoostingClassifier(class_weight='balanced', random_state=315)\n",
    "\n",
    "        # Set-up the search\n",
    "        classification_search=RandomizedSearchCV(\n",
    "            classification_model,\n",
    "            distributions,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=cpus,\n",
    "            cv=cross_validation,\n",
    "            n_iter=randomsearch_depth,\n",
    "            random_state=315,\n",
    "            return_train_score=True\n",
    "        )\n",
    "\n",
    "        classification_search_results=classification_search.fit(training_features_df, training_labels_df['efs'])\n",
    "        \n",
    "        print(f'\\n{dataset}:')\n",
    "        for parameter, value in classification_search_results.best_params_.items():\n",
    "            print(f' {parameter}: {value}')\n",
    "\n",
    "        # Train classifier with best hyperparameters on complete training set\n",
    "        classification_model=HistGradientBoostingClassifier(**classification_search_results.best_params_, random_state=315)\n",
    "        result=classification_model.fit(training_features_df, training_labels_df['efs'])\n",
    "        classification_models[dataset]=classification_model\n",
    "\n",
    "        # Make testing predictions\n",
    "        testing_features_df=data_dict['Testing features']\n",
    "        classification_testing_predictions=classification_model.predict(testing_features_df)\n",
    "        classification_predictions['Testing'][dataset]=classification_testing_predictions\n",
    "\n",
    "    classification_test_results={'Testing predictions': classification_predictions}\n",
    "    classification_test_results['Tuned models']=classification_models\n",
    "\n",
    "    with open(classification_test_results_file, 'wb') as output_file:\n",
    "        pickle.dump(classification_test_results, output_file)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load last result\n",
    "    with open(classification_test_results_file, 'rb') as input_file:\n",
    "        classification_test_results=pickle.load(input_file)\n",
    "\n",
    "    classification_predictions=classification_test_results['Testing predictions']\n",
    "    classification_models=classification_test_results['Tuned models']\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Results\n",
    "\n",
    "#### 3.2.1. Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,3, figsize=(8,5.5), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('SciKit-learn gradient boosting\\nclassifier performance: hold-out test set')\n",
    "\n",
    "for i, dataset in enumerate(classification_predictions['Testing'].keys()):\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    cm=confusion_matrix(data_dict['Testing labels']['efs'], classification_predictions['Testing'][dataset], normalize='true')\n",
    "    cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    _=cm_disp.plot(ax=axs[i])\n",
    "\n",
    "    axs[i].set_title(dataset.replace(', ', '\\n').replace('/', '\\n'))\n",
    "    axs[i].set_xlabel('Predicted EFS')\n",
    "    axs[i].set_ylabel('True EFS')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(classification_test_performance_plots, dpi=300, bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,3, figsize=(8,5.5), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "fig.suptitle('SciKit-learn gradient boosting\\nclassifier probabilities: hold-out test set')\n",
    "\n",
    "for i, dataset in enumerate(classification_predictions['Testing'].keys()):\n",
    "\n",
    "    # Load the data\n",
    "    data_file=datasets[dataset]\n",
    "\n",
    "    with open(data_file, 'rb') as input_file:\n",
    "        data_dict=pickle.load(input_file)\n",
    "\n",
    "    class_probabilities=classification_models[dataset].predict_proba(data_dict['Testing features'])\n",
    "    class_df=pd.DataFrame.from_dict({\n",
    "        'EFS': data_dict['Testing labels']['efs'].values,\n",
    "        'EFS probability': class_probabilities[:,0]\n",
    "    })\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    sns.histplot(class_df, x='EFS probability', hue='EFS', ax=axs[i])\n",
    "\n",
    "    axs[i].set_title(dataset.replace(', ', '\\n').replace('/', '\\n'))\n",
    "    axs[i].set_xlabel('Predicted EFS')\n",
    "    axs[i].set_ylabel('True EFS')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(classification_test_probability_plots, dpi=300, bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
